{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "located-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  7 13:42:41 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Tesla V1...  On   | 00000000:37:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    26W / 250W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA Tesla V1...  On   | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   73C    P0   134W / 250W |  15729MiB / 16160MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A      5642      C   python3                         15725MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-effort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASRModel(\n",
      "  (conv1d_layer): Conv1d(40, 256, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "  (lstm_block): LSTM(256, 256, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (output_layer): Linear(in_features=512, out_features=29, bias=True)\n",
      ")\n",
      "Trainable parameters: 2695965\n",
      "\u001b[1m\n",
      "Epoch 1\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=1.091, WER=100.00, CER=100.79\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.854, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 2\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.844, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.853, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 3\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.2 min(s)]: Loss=0.843, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.853, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 4\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.842, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.853, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 5\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.841, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.852, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 6\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.838, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.846, WER=100.00, CER=99.97\n",
      "\u001b[1m\n",
      "Epoch 7\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.826, WER=100.00, CER=90.26\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.828, WER=100.00, CER=86.65\n",
      "\u001b[1m\n",
      "Epoch 8\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.811, WER=100.00, CER=84.70\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.816, WER=100.00, CER=85.19\n",
      "\u001b[1m\n",
      "Epoch 9\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.802, WER=100.00, CER=84.04\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.810, WER=100.00, CER=84.88\n",
      "\u001b[1m\n",
      "Epoch 10\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.795, WER=100.00, CER=83.79\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.805, WER=100.00, CER=84.59\n",
      "\u001b[1m\n",
      "Epoch 11\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.790, WER=100.00, CER=83.56\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.801, WER=100.00, CER=84.47\n",
      "\u001b[1m\n",
      "Epoch 12\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.785, WER=100.00, CER=82.96\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.797, WER=99.97, CER=83.16\n",
      "\u001b[1m\n",
      "Epoch 13\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.780, WER=99.85, CER=79.55\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.794, WER=99.64, CER=78.78\n",
      "\u001b[1m\n",
      "Epoch 14\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.776, WER=101.45, CER=76.54\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.791, WER=99.95, CER=77.84\n",
      "\u001b[1m\n",
      "Epoch 15\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.774, WER=103.02, CER=75.80\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.785, WER=99.90, CER=76.95\n",
      "\u001b[1m\n",
      "Epoch 16\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.771, WER=104.10, CER=75.29\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.781, WER=99.90, CER=76.26\n",
      "\u001b[1m\n",
      "Epoch 17\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.770, WER=103.74, CER=74.68\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.780, WER=100.01, CER=74.92\n",
      "\u001b[1m\n",
      "Epoch 18\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.2 min(s)]: Loss=0.764, WER=106.30, CER=73.13\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.775, WER=101.05, CER=74.21\n",
      "\u001b[1m\n",
      "Epoch 19\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.2 min(s)]: Loss=0.761, WER=108.76, CER=72.55\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.770, WER=101.66, CER=73.54\n",
      "\u001b[1m\n",
      "Epoch 20\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.759, WER=110.12, CER=72.38\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.769, WER=102.07, CER=73.57\n",
      "\u001b[1m\n",
      "Epoch 21\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.758, WER=111.30, CER=72.25\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.769, WER=102.16, CER=73.67\n",
      "\u001b[1m\n",
      "Epoch 22\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.756, WER=111.96, CER=72.08\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.766, WER=102.76, CER=73.23\n",
      "\u001b[1m\n",
      "Epoch 23\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.755, WER=112.56, CER=72.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.766, WER=102.47, CER=73.55\n",
      "\u001b[1m\n",
      "Epoch 24\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.754, WER=112.44, CER=71.83\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=104.63, CER=73.00\n",
      "\u001b[1m\n",
      "Epoch 25\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.753, WER=112.94, CER=71.62\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=103.92, CER=72.55\n",
      "\u001b[1m\n",
      "Epoch 26\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.751, WER=113.66, CER=71.67\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.760, WER=105.35, CER=72.83\n",
      "\u001b[1m\n",
      "Epoch 27\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.751, WER=112.73, CER=71.70\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=103.57, CER=73.41\n",
      "\u001b[1m\n",
      "Epoch 28\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.752, WER=110.93, CER=71.89\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.773, WER=101.61, CER=75.58\n",
      "\u001b[1m\n",
      "Epoch 29\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.754, WER=110.21, CER=72.27\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=104.09, CER=73.58\n",
      "\u001b[1m\n",
      "Epoch 30\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.753, WER=111.43, CER=72.11\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=102.95, CER=74.52\n",
      "\u001b[1m\n",
      "Epoch 31\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.751, WER=113.43, CER=71.80\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=103.49, CER=73.25\n",
      "\u001b[1m\n",
      "Epoch 32\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.750, WER=113.82, CER=71.70\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=104.07, CER=73.36\n",
      "\u001b[1m\n",
      "Epoch 33\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.749, WER=114.27, CER=71.69\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=105.90, CER=72.89\n",
      "\u001b[1m\n",
      "Epoch 34\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.749, WER=115.16, CER=71.52\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.761, WER=105.92, CER=72.57\n",
      "\u001b[1m\n",
      "Epoch 35\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.747, WER=116.00, CER=71.43\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.760, WER=106.09, CER=72.66\n",
      "\u001b[1m\n",
      "Epoch 36\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.746, WER=115.38, CER=71.41\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.760, WER=107.34, CER=72.49\n",
      "\u001b[1m\n",
      "Epoch 37\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.747, WER=114.30, CER=71.42\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.760, WER=109.31, CER=71.38\n",
      "\u001b[1m\n",
      "Epoch 38\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.749, WER=108.32, CER=71.87\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.770, WER=116.00, CER=70.40\n",
      "\u001b[1m\n",
      "Epoch 39\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.751, WER=110.40, CER=71.87\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.759, WER=105.28, CER=73.07\n",
      "\u001b[1m\n",
      "Epoch 40\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.748, WER=113.76, CER=71.47\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.759, WER=108.68, CER=71.55\n",
      "\u001b[1m\n",
      "Epoch 41\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.748, WER=110.34, CER=71.73\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.758, WER=110.61, CER=71.13\n",
      "\u001b[1m\n",
      "Epoch 42\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.749, WER=109.45, CER=71.85\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.761, WER=110.46, CER=71.21\n",
      "\u001b[1m\n",
      "Epoch 43\u001b[0m\n",
      "Running training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [63/63, 0.4 min(s)]: Loss=0.749, WER=108.07, CER=72.11\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.759, WER=109.55, CER=71.07\n",
      "\u001b[1m\n",
      "Epoch 44\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.749, WER=109.87, CER=72.07\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.760, WER=105.21, CER=72.35\n",
      "\u001b[1m\n",
      "Epoch 45\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.748, WER=108.91, CER=72.11\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.757, WER=108.32, CER=71.84\n",
      "\u001b[1m\n",
      "Epoch 46\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.747, WER=110.26, CER=71.69\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.758, WER=112.82, CER=71.14\n",
      "\u001b[1m\n",
      "Epoch 47\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.747, WER=110.80, CER=71.68\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.758, WER=105.51, CER=72.84\n",
      "\u001b[1m\n",
      "Epoch 48\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.746, WER=110.42, CER=71.64\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.759, WER=105.06, CER=72.78\n",
      "\u001b[1m\n",
      "Epoch 49\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.747, WER=112.52, CER=71.39\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=105.52, CER=73.09\n",
      "\u001b[1m\n",
      "Epoch 50\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.749, WER=112.98, CER=71.86\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.759, WER=105.32, CER=73.08\n",
      "\u001b[1m\n",
      "Epoch 51\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.748, WER=110.92, CER=72.04\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.759, WER=105.11, CER=72.76\n",
      "\u001b[1m\n",
      "Epoch 52\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.747, WER=111.73, CER=71.79\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.758, WER=105.77, CER=72.36\n",
      "\u001b[1m\n",
      "Epoch 53\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.746, WER=111.21, CER=71.72\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.756, WER=106.62, CER=72.21\n",
      "\u001b[1m\n",
      "Epoch 54\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.744, WER=110.81, CER=71.51\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.757, WER=105.49, CER=72.70\n",
      "\u001b[1m\n",
      "Epoch 55\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.2 min(s)]: Loss=0.744, WER=109.90, CER=71.64\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.758, WER=103.62, CER=72.73\n",
      "\u001b[1m\n",
      "Epoch 56\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.744, WER=110.64, CER=71.61\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.758, WER=104.11, CER=72.81\n",
      "\u001b[1m\n",
      "Epoch 57\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.743, WER=111.40, CER=71.49\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.757, WER=104.54, CER=72.52\n",
      "\u001b[1m\n",
      "Epoch 58\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.743, WER=111.58, CER=71.38\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.757, WER=105.95, CER=72.35\n",
      "\u001b[1m\n",
      "Epoch 59\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.743, WER=112.06, CER=71.42\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.757, WER=104.32, CER=72.85\n",
      "\u001b[1m\n",
      "Epoch 60\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.742, WER=111.46, CER=71.49\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.757, WER=104.00, CER=72.78\n",
      "\u001b[1m\n",
      "Epoch 61\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.741, WER=111.90, CER=71.37\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.759, WER=103.52, CER=72.97\n",
      "\u001b[1m\n",
      "Epoch 62\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.740, WER=111.90, CER=71.31\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.758, WER=103.96, CER=73.00\n",
      "\u001b[1m\n",
      "Epoch 63\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.741, WER=112.34, CER=71.32\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.757, WER=105.40, CER=72.89\n",
      "\u001b[1m\n",
      "Epoch 64\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.740, WER=112.41, CER=71.17\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.757, WER=105.93, CER=72.60\n",
      "\u001b[1m\n",
      "Epoch 65\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.738, WER=112.87, CER=71.04\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.757, WER=105.79, CER=72.37\n",
      "\u001b[1m\n",
      "Epoch 66\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.737, WER=112.12, CER=71.07\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.758, WER=106.67, CER=72.06\n",
      "\u001b[1m\n",
      "Epoch 67\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.738, WER=111.76, CER=71.14\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.760, WER=104.46, CER=72.60\n",
      "\u001b[1m\n",
      "Epoch 68\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.737, WER=111.81, CER=71.12\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.759, WER=104.96, CER=72.70\n",
      "\u001b[1m\n",
      "Epoch 69\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.2 min(s)]: Loss=0.736, WER=111.29, CER=70.99\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.758, WER=104.56, CER=72.36\n",
      "\u001b[1m\n",
      "Epoch 70\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.2 min(s)]: Loss=0.735, WER=110.92, CER=71.09\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.760, WER=105.69, CER=72.34\n",
      "\u001b[1m\n",
      "Epoch 71\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.735, WER=111.10, CER=71.02\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.760, WER=104.36, CER=72.71\n",
      "\u001b[1m\n",
      "Epoch 72\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.735, WER=111.75, CER=70.93\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.761, WER=105.11, CER=72.52\n",
      "\u001b[1m\n",
      "Epoch 73\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.734, WER=111.84, CER=70.92\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.760, WER=105.22, CER=72.11\n",
      "\u001b[1m\n",
      "Epoch 74\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.733, WER=110.45, CER=70.96\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.759, WER=107.21, CER=71.69\n",
      "\u001b[1m\n",
      "Epoch 75\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.732, WER=111.17, CER=70.72\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.761, WER=105.34, CER=72.08\n",
      "\u001b[1m\n",
      "Epoch 76\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.733, WER=111.10, CER=70.76\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=107.80, CER=72.09\n",
      "\u001b[1m\n",
      "Epoch 77\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.738, WER=111.32, CER=71.07\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=105.22, CER=72.38\n",
      "\u001b[1m\n",
      "Epoch 78\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.733, WER=111.40, CER=70.76\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=105.08, CER=72.27\n",
      "\u001b[1m\n",
      "Epoch 79\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.731, WER=110.63, CER=70.75\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=105.02, CER=71.94\n",
      "\u001b[1m\n",
      "Epoch 80\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.730, WER=110.38, CER=70.77\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=106.18, CER=72.23\n",
      "\u001b[1m\n",
      "Epoch 81\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.727, WER=111.33, CER=70.58\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=104.27, CER=72.17\n",
      "\u001b[1m\n",
      "Epoch 82\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.726, WER=110.31, CER=70.55\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=105.84, CER=71.80\n",
      "\u001b[1m\n",
      "Epoch 83\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.726, WER=110.24, CER=70.54\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=104.74, CER=71.91\n",
      "\u001b[1m\n",
      "Epoch 84\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.2 min(s)]: Loss=0.723, WER=109.34, CER=70.51\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=105.18, CER=71.76\n",
      "\u001b[1m\n",
      "Epoch 85\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.722, WER=110.17, CER=70.22\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.766, WER=105.02, CER=71.55\n",
      "\u001b[1m\n",
      "Epoch 86\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.720, WER=109.81, CER=70.15\n",
      "Running evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=104.85, CER=71.87\n",
      "\u001b[1m\n",
      "Epoch 87\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.718, WER=110.04, CER=69.98\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.768, WER=104.15, CER=71.98\n",
      "\u001b[1m\n",
      "Epoch 88\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.718, WER=110.38, CER=69.94\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.769, WER=103.21, CER=72.40\n",
      "\u001b[1m\n",
      "Epoch 89\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.716, WER=110.28, CER=69.84\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.769, WER=103.76, CER=72.22\n",
      "\u001b[1m\n",
      "Epoch 90\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.713, WER=109.93, CER=69.78\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.770, WER=104.29, CER=71.63\n",
      "\u001b[1m\n",
      "Epoch 91\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.712, WER=109.56, CER=69.66\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.769, WER=104.33, CER=71.56\n",
      "\u001b[1m\n",
      "Epoch 92\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.710, WER=109.73, CER=69.44\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.772, WER=104.39, CER=71.81\n",
      "\u001b[1m\n",
      "Epoch 93\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.708, WER=109.15, CER=69.43\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.769, WER=106.80, CER=71.11\n",
      "\u001b[1m\n",
      "Epoch 94\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.706, WER=108.75, CER=69.37\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.774, WER=104.04, CER=71.76\n",
      "\u001b[1m\n",
      "Epoch 95\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.704, WER=108.57, CER=69.12\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.774, WER=104.86, CER=71.50\n",
      "\u001b[1m\n",
      "Epoch 96\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.702, WER=108.34, CER=69.02\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.774, WER=105.47, CER=71.37\n",
      "\u001b[1m\n",
      "Epoch 97\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.700, WER=108.42, CER=68.83\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.776, WER=105.37, CER=71.25\n",
      "\u001b[1m\n",
      "Epoch 98\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.697, WER=108.25, CER=68.75\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.777, WER=104.86, CER=71.44\n",
      "\u001b[1m\n",
      "Epoch 99\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.695, WER=107.79, CER=68.58\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.779, WER=105.38, CER=71.21\n",
      "\u001b[1m\n",
      "Epoch 100\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.696, WER=107.30, CER=68.42\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.778, WER=104.55, CER=71.49\n",
      "\u001b[1m\n",
      "Epoch 101\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.692, WER=107.29, CER=68.31\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.782, WER=104.88, CER=71.71\n",
      "\u001b[1m\n",
      "Epoch 102\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.688, WER=107.22, CER=68.07\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.783, WER=106.09, CER=70.95\n",
      "\u001b[1m\n",
      "Epoch 103\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.688, WER=107.24, CER=67.91\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.784, WER=104.35, CER=71.35\n",
      "\u001b[1m\n",
      "Epoch 104\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.684, WER=106.90, CER=67.76\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.787, WER=103.70, CER=71.54\n",
      "\u001b[1m\n",
      "Epoch 105\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.682, WER=106.20, CER=67.62\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.789, WER=103.25, CER=71.39\n",
      "\u001b[1m\n",
      "Epoch 106\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.679, WER=106.21, CER=67.53\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.787, WER=104.00, CER=71.46\n",
      "\u001b[1m\n",
      "Epoch 107\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.680, WER=106.32, CER=67.52\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.785, WER=106.50, CER=70.72\n",
      "\u001b[1m\n",
      "Epoch 108\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.683, WER=106.51, CER=67.62\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.787, WER=108.03, CER=70.34\n",
      "\u001b[1m\n",
      "Epoch 109\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.676, WER=105.98, CER=67.25\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.787, WER=109.03, CER=70.06\n",
      "\u001b[1m\n",
      "Epoch 110\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.671, WER=105.64, CER=66.99\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.789, WER=108.63, CER=70.05\n",
      "\u001b[1m\n",
      "Epoch 111\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.2 min(s)]: Loss=0.668, WER=105.17, CER=66.82\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.792, WER=110.29, CER=69.73\n",
      "\u001b[1m\n",
      "Epoch 112\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.667, WER=105.52, CER=66.63\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.794, WER=113.99, CER=68.87\n",
      "\u001b[1m\n",
      "Epoch 113\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.665, WER=105.28, CER=66.40\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.796, WER=112.90, CER=68.54\n",
      "\u001b[1m\n",
      "Epoch 114\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.661, WER=104.55, CER=66.13\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.3 min(s)]: Loss=0.795, WER=111.97, CER=68.54\n",
      "\u001b[1m\n",
      "Epoch 115\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.659, WER=104.34, CER=65.98\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.805, WER=113.97, CER=67.95\n",
      "\u001b[1m\n",
      "Epoch 116\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.658, WER=104.12, CER=65.75\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.810, WER=114.06, CER=68.18\n",
      "\u001b[1m\n",
      "Epoch 117\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.656, WER=104.36, CER=65.68\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.815, WER=113.04, CER=68.09\n",
      "\u001b[1m\n",
      "Epoch 118\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.650, WER=104.27, CER=65.21\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.816, WER=112.16, CER=68.27\n",
      "\u001b[1m\n",
      "Epoch 119\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.645, WER=104.08, CER=64.99\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.820, WER=113.36, CER=68.28\n",
      "\u001b[1m\n",
      "Epoch 120\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.639, WER=103.39, CER=64.55\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.821, WER=112.56, CER=68.32\n",
      "\u001b[1m\n",
      "Epoch 121\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.635, WER=102.98, CER=64.34\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.824, WER=112.46, CER=68.48\n",
      "\u001b[1m\n",
      "Epoch 122\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.631, WER=102.88, CER=64.04\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.826, WER=111.36, CER=68.52\n",
      "\u001b[1m\n",
      "Epoch 123\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.628, WER=102.40, CER=63.84\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.826, WER=110.94, CER=68.64\n",
      "\u001b[1m\n",
      "Epoch 124\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.626, WER=102.35, CER=63.48\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.826, WER=110.80, CER=68.92\n",
      "\u001b[1m\n",
      "Epoch 125\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.623, WER=101.94, CER=63.40\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.832, WER=111.20, CER=68.93\n",
      "\u001b[1m\n",
      "Epoch 126\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.620, WER=102.14, CER=63.06\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.830, WER=110.36, CER=68.86\n",
      "\u001b[1m\n",
      "Epoch 127\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.615, WER=101.56, CER=62.79\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.836, WER=110.47, CER=68.92\n",
      "\u001b[1m\n",
      "Epoch 128\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.613, WER=101.56, CER=62.62\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.832, WER=109.21, CER=68.95\n",
      "\u001b[1m\n",
      "Epoch 129\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.609, WER=101.18, CER=62.29\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.839, WER=110.80, CER=69.11\n",
      "\u001b[1m\n",
      "Epoch 130\u001b[0m\n",
      "Running training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [63/63, 0.5 min(s)]: Loss=0.607, WER=100.79, CER=62.08\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.835, WER=109.28, CER=69.12\n",
      "\u001b[1m\n",
      "Epoch 131\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.603, WER=100.75, CER=61.80\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.838, WER=109.73, CER=69.21\n",
      "\u001b[1m\n",
      "Epoch 132\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.600, WER=101.08, CER=61.59\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.839, WER=108.33, CER=69.06\n",
      "\u001b[1m\n",
      "Epoch 133\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.597, WER=100.42, CER=61.45\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.838, WER=107.79, CER=69.19\n",
      "\u001b[1m\n",
      "Epoch 134\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.593, WER=100.26, CER=60.97\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.843, WER=108.12, CER=69.43\n",
      "\u001b[1m\n",
      "Epoch 135\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.591, WER=100.07, CER=60.96\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.843, WER=107.19, CER=69.34\n",
      "\u001b[1m\n",
      "Epoch 136\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.586, WER=99.89, CER=60.53 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.843, WER=107.16, CER=69.31\n",
      "\u001b[1m\n",
      "Epoch 137\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.584, WER=99.60, CER=60.28 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.844, WER=106.36, CER=69.50\n",
      "\u001b[1m\n",
      "Epoch 138\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.584, WER=99.83, CER=60.34 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.844, WER=105.85, CER=69.47\n",
      "\u001b[1m\n",
      "Epoch 139\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.581, WER=99.48, CER=60.07 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.849, WER=106.39, CER=69.37\n",
      "\u001b[1m\n",
      "Epoch 140\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.576, WER=99.14, CER=59.66 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.851, WER=105.02, CER=69.55\n",
      "\u001b[1m\n",
      "Epoch 141\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.574, WER=98.94, CER=59.49 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.856, WER=105.77, CER=69.73\n",
      "\u001b[1m\n",
      "Epoch 142\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.572, WER=99.23, CER=59.22 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.856, WER=105.28, CER=69.69\n",
      "\u001b[1m\n",
      "Epoch 143\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.566, WER=98.92, CER=59.01 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.859, WER=105.44, CER=69.76\n",
      "\u001b[1m\n",
      "Epoch 144\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.3 min(s)]: Loss=0.566, WER=98.81, CER=58.78 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.864, WER=106.04, CER=70.06\n",
      "\u001b[1m\n",
      "Epoch 145\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.562, WER=98.34, CER=58.60\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.863, WER=105.25, CER=70.10\n",
      "\u001b[1m\n",
      "Epoch 146\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.563, WER=98.39, CER=58.63 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.870, WER=106.17, CER=70.37\n",
      "\u001b[1m\n",
      "Epoch 147\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.558, WER=97.98, CER=58.20\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.873, WER=106.20, CER=70.37\n",
      "\u001b[1m\n",
      "Epoch 148\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.556, WER=98.13, CER=58.17 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.875, WER=105.96, CER=70.67\n",
      "\u001b[1m\n",
      "Epoch 149\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.552, WER=97.94, CER=57.82\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.880, WER=105.54, CER=70.83\n",
      "\u001b[1m\n",
      "Epoch 150\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.2 min(s)]: Loss=0.551, WER=97.89, CER=57.61\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.878, WER=105.09, CER=70.67\n",
      "\u001b[1m\n",
      "Epoch 151\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.549, WER=97.86, CER=57.49\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.878, WER=104.62, CER=70.43\n",
      "\u001b[1m\n",
      "Epoch 152\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.545, WER=97.39, CER=57.07\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.880, WER=103.83, CER=70.33\n",
      "\u001b[1m\n",
      "Epoch 153\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.543, WER=97.31, CER=56.91\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.881, WER=103.72, CER=70.24\n",
      "\u001b[1m\n",
      "Epoch 154\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.541, WER=97.42, CER=56.84\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.883, WER=103.38, CER=70.17\n",
      "\u001b[1m\n",
      "Epoch 155\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.535, WER=97.08, CER=56.25\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.885, WER=103.85, CER=70.20\n",
      "\u001b[1m\n",
      "Epoch 156\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.4 min(s)]: Loss=0.532, WER=96.77, CER=55.92\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.888, WER=103.23, CER=70.14\n",
      "\u001b[1m\n",
      "Epoch 157\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.527, WER=96.21, CER=55.60\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.890, WER=103.31, CER=70.22\n",
      "\u001b[1m\n",
      "Epoch 158\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.524, WER=96.21, CER=55.27\n",
      "Running evaluation:\n",
      "Validation [102/169, 0.7 min(s)]: Loss=0.886, WER=103.50, CER=70.18\r"
     ]
    }
   ],
   "source": [
    "######################################### ASR not-pretained main script #################################################\n",
    "\n",
    "# Import general libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from datetime import datetime\n",
    "\n",
    "import editdistance\n",
    "\n",
    "# Import stuff from other ASR modules\n",
    "from asr.data import BaseDataset\n",
    "from asr.data.preprocessors import SpectrogramPreprocessor, TextPreprocessor\n",
    "from asr.modules import ASRModel\n",
    "from asr.utils.training import batch_to_tensor, epochs, Logger\n",
    "from asr.utils.text import greedy_ctc\n",
    "from asr.utils.metrics import ErrorRateTracker, LossTracker\n",
    "\n",
    "def get_Stats(ref_batch,hyp_batch):\n",
    "    ref_WER_splits = [rf.split() for rf in ref_batch]\n",
    "    hyp_WER_splits = [hp.split() for hp in hyp_batch]\n",
    "    ref_WER_lens = [len(rf_spl) for rf_spl in ref_WER_splits]\n",
    "    hyp_WER_lens = [len(hp_spl) for hp_spl in hyp_WER_splits]\n",
    "    \n",
    "    WER = [editdistance.eval(rf,hp) for rf, hp in zip(ref_WER_splits, hyp_WER_splits)]\n",
    "    \n",
    "    ref_CER_splits = [list(rf) for rf in ref_batch]\n",
    "    hyp_CER_splits = [list(hp) for hp in hyp_batch]\n",
    "    ref_CER_lens = [len(rf_spl) for rf_spl in ref_CER_splits]\n",
    "    hyp_CER_lens = [len(hp_spl) for hp_spl in hyp_CER_splits]\n",
    "    \n",
    "    CER = [editdistance.eval(rf,hp) for rf, hp in zip(ref_CER_splits, hyp_CER_splits)]\n",
    "    \n",
    "    return WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens\n",
    "\n",
    "\n",
    "\"\"\" Function: Train and test ASR model on input datasets\n",
    "    Input:    2 txt-files with IDs to training and test files. One observation consists of a transcript\n",
    "              (txt-feature) and audio file (wav-target).\n",
    "    Output:   Return best WER (validation) and save ASR model (model.pt) \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Part 1: Load and preprocess data \"\"\"\n",
    "train_IDs = 'wavenet-cut'\n",
    "test_IDs = 'dev-clean'\n",
    "\n",
    "\n",
    "train_source = train_IDs\n",
    "val_source = test_IDs\n",
    "\n",
    "# BLACK BOX\n",
    "spec_preprocessor = SpectrogramPreprocessor(output_format='NFT', sample_rate=4000)\n",
    "text_preprocessor = TextPreprocessor()\n",
    "preprocessor = [spec_preprocessor, text_preprocessor]\n",
    "\n",
    "train_dataset = BaseDataset(source=train_source, preprocessor=preprocessor, sort_by=0)\n",
    "val_dataset = BaseDataset(source=val_source, preprocessor=preprocessor, sort_by=0)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(train_dataset, num_workers=4, pin_memory=True, collate_fn=train_dataset.collate, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, num_workers=4, pin_memory=True, collate_fn=val_dataset.collate, batch_size=16)\n",
    "\n",
    "\"\"\" Part 2: Setup model and loss \"\"\"\n",
    "# Create instance of model\n",
    "asr_model = ASRModel(input_size=40).cuda()\n",
    "print(asr_model)\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in asr_model.parameters() if p.requires_grad))\n",
    "\n",
    "# Define loss, optimizer and learning rate scheduler\n",
    "ctc_loss = nn.CTCLoss(reduction='none').cuda()#reduction='sum'\n",
    "optimizer = torch.optim.Adam(asr_model.parameters(), lr=3e-4)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=5e-5)\n",
    "\n",
    "\"\"\" Part 3: Train and evaluate model \"\"\"\n",
    "# Variables to create and store performance metrics\n",
    "wer_metric = ErrorRateTracker(word_based=True)\n",
    "cer_metric = ErrorRateTracker(word_based=False)\n",
    "ctc_metric = LossTracker()\n",
    "\n",
    "train_logger = Logger('Training', ctc_metric, wer_metric, cer_metric)\n",
    "val_logger = Logger('Validation', ctc_metric, wer_metric, cer_metric)\n",
    "\n",
    "def forward_pass(batch, training=False):\n",
    "    (x, x_sl), (y, y_sl) = batch_to_tensor(batch)  # For CPU: change 'cuda' to 'cpu'\n",
    "    \n",
    "    logits, output_sl = asr_model.forward(x, x_sl.cpu())\n",
    "    log_probs = F.log_softmax(logits, dim=2)\n",
    "    loss = ctc_loss(log_probs, y, output_sl, y_sl)\n",
    "    \n",
    "    hyp_encoded_batch = greedy_ctc(logits, output_sl)\n",
    "    hyp_batch = text_preprocessor.decode_batch(hyp_encoded_batch)\n",
    "    ref_batch = text_preprocessor.decode_batch(y, y_sl)\n",
    "    \n",
    "    wer_metric.update(ref_batch, hyp_batch)\n",
    "    cer_metric.update(ref_batch, hyp_batch)\n",
    "    ctc_metric.update(loss.sum().item(), weight=output_sl.sum().item())\n",
    "\n",
    "    if not training:\n",
    "        WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens = get_Stats(ref_batch,hyp_batch)\n",
    "        CTC_loss = [ctc.item() for ctc in loss]\n",
    "        return CTC_loss, WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "# Run 200 epochs\n",
    "model_name = f\"{train_IDs}vs{test_IDs}{datetime.now().strftime('Y%Y-m%m-d%d-H%H-M%M')}\" \n",
    "with open(f\"./results/{model_name}.csv\", \"a+\") as o_f:\n",
    "    for epoch in epochs(200):\n",
    "        # Set PyTorch in training mode\n",
    "        asr_model.train()\n",
    "\n",
    "        # Train model on training set\n",
    "        print(\"Running training:\")\n",
    "        for batch, files in train_logger(train_loader):\n",
    "            loss = forward_pass(batch, training=True)\n",
    "            optimizer.zero_grad()\n",
    "            loss.sum().backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Set PyTorch in test mode\n",
    "        asr_model.eval()\n",
    "\n",
    "        # Test model using test set\n",
    "        print(\"Running evaluation:\")\n",
    "        N = 1\n",
    "        wer_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, files in val_logger(val_loader):\n",
    "                CTC_loss, WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens = forward_pass(batch, training=False)\n",
    "                wer_sum += np.sum(WER)\n",
    "                N += len(WER)\n",
    "                for i, flac_file in enumerate(files):\n",
    "                    idx = os.path.splitext(os.path.basename(flac_file))[0]\n",
    "                    print(f\"{epoch}\\t{idx}\\t{CTC_loss[i]}\\t{WER[i]}\\t{ref_WER_lens[i]}\\t{hyp_WER_lens[i]}\\t{CER[i]}\\t{ref_CER_lens[i]}\\t{hyp_CER_lens[i]}\", file=o_f)\n",
    "\n",
    "        best_wer = np.inf\n",
    "        if wer_sum/N < best_wer:\n",
    "            best_wer = wer_sum/N\n",
    "            torch.save(asr_model.state_dict(), f\"./results/best_{model_name}.pt\")\n",
    "\n",
    "        if epoch >= 100:\n",
    "            lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-territory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "located-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  7 13:42:49 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Tesla V1...  On   | 00000000:37:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    26W / 250W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA Tesla V1...  On   | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   73C    P0   213W / 250W |  15729MiB / 16160MiB |     75%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A      5642      C   python3                         15725MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-effort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASRModel(\n",
      "  (conv1d_layer): Conv1d(40, 256, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "  (lstm_block): LSTM(256, 256, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (output_layer): Linear(in_features=512, out_features=29, bias=True)\n",
      ")\n",
      "Trainable parameters: 2695965\n",
      "\u001b[1m\n",
      "Epoch 1\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=1.270, WER=99.99, CER=99.84\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.858, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 2\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.935, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.853, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 3\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.928, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.850, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 4\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.913, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.841, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 5\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.887, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.827, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 6\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.854, WER=100.00, CER=97.81\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.802, WER=100.00, CER=91.77\n",
      "\u001b[1m\n",
      "Epoch 7\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.808, WER=99.99, CER=86.98 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.779, WER=99.94, CER=83.91\n",
      "\u001b[1m\n",
      "Epoch 8\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.757, WER=99.33, CER=79.12\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.758, WER=99.14, CER=80.17\n",
      "\u001b[1m\n",
      "Epoch 9\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.710, WER=98.52, CER=71.88\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.745, WER=98.03, CER=75.05\n",
      "\u001b[1m\n",
      "Epoch 10\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.666, WER=100.01, CER=64.76\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.736, WER=98.38, CER=71.39\n",
      "\u001b[1m\n",
      "Epoch 11\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.628, WER=100.75, CER=59.92\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.727, WER=99.21, CER=68.78\n",
      "\u001b[1m\n",
      "Epoch 12\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.592, WER=100.79, CER=56.46\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.725, WER=98.98, CER=67.92\n",
      "\u001b[1m\n",
      "Epoch 13\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.560, WER=99.22, CER=53.78 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.724, WER=98.98, CER=66.79\n",
      "\u001b[1m\n",
      "Epoch 14\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.530, WER=98.07, CER=51.08\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.720, WER=99.10, CER=65.95\n",
      "\u001b[1m\n",
      "Epoch 15\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.503, WER=95.60, CER=48.75\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.721, WER=99.72, CER=64.80\n",
      "\u001b[1m\n",
      "Epoch 16\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.476, WER=93.44, CER=46.01\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.727, WER=101.66, CER=63.76\n",
      "\u001b[1m\n",
      "Epoch 17\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.454, WER=91.76, CER=43.84\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.731, WER=102.63, CER=63.26\n",
      "\u001b[1m\n",
      "Epoch 18\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.431, WER=89.34, CER=41.60\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.745, WER=100.99, CER=63.31\n",
      "\u001b[1m\n",
      "Epoch 19\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.408, WER=86.89, CER=39.58\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.744, WER=102.61, CER=62.44\n",
      "\u001b[1m\n",
      "Epoch 20\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.388, WER=84.57, CER=37.61\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.755, WER=99.45, CER=62.86\n",
      "\u001b[1m\n",
      "Epoch 21\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.369, WER=82.55, CER=35.90\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.755, WER=99.24, CER=62.82\n",
      "\u001b[1m\n",
      "Epoch 22\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.349, WER=80.32, CER=33.85\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=99.58, CER=62.25\n",
      "\u001b[1m\n",
      "Epoch 23\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.331, WER=78.64, CER=32.24\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.779, WER=99.46, CER=62.30\n",
      "\u001b[1m\n",
      "Epoch 24\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.315, WER=76.27, CER=30.62\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.795, WER=99.13, CER=62.31\n",
      "\u001b[1m\n",
      "Epoch 25\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.298, WER=73.90, CER=28.95\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.813, WER=99.15, CER=62.38\n",
      "\u001b[1m\n",
      "Epoch 26\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.283, WER=72.21, CER=27.52\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.825, WER=99.83, CER=62.15\n",
      "\u001b[1m\n",
      "Epoch 27\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.270, WER=70.33, CER=26.12\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.841, WER=101.20, CER=62.10\n",
      "\u001b[1m\n",
      "Epoch 28\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.260, WER=69.07, CER=25.30\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.853, WER=103.78, CER=62.12\n",
      "\u001b[1m\n",
      "Epoch 29\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.254, WER=69.36, CER=24.89\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.866, WER=99.16, CER=62.52\n",
      "\u001b[1m\n",
      "Epoch 30\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.244, WER=67.19, CER=23.84\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.866, WER=99.20, CER=61.95\n",
      "\u001b[1m\n",
      "Epoch 31\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.229, WER=64.72, CER=22.56\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.870, WER=97.87, CER=62.19\n",
      "\u001b[1m\n",
      "Epoch 32\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.214, WER=61.57, CER=20.84\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.892, WER=98.14, CER=62.39\n",
      "\u001b[1m\n",
      "Epoch 33\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.200, WER=59.44, CER=19.41\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.906, WER=98.04, CER=62.82\n",
      "\u001b[1m\n",
      "Epoch 34\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.188, WER=56.65, CER=18.34\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.908, WER=98.37, CER=61.95\n",
      "\u001b[1m\n",
      "Epoch 35\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.176, WER=54.68, CER=16.99\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.925, WER=98.47, CER=61.85\n",
      "\u001b[1m\n",
      "Epoch 36\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.166, WER=53.15, CER=16.20\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.947, WER=98.91, CER=62.09\n",
      "\u001b[1m\n",
      "Epoch 37\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.160, WER=51.17, CER=15.44\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.975, WER=98.94, CER=62.58\n",
      "\u001b[1m\n",
      "Epoch 38\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.151, WER=49.46, CER=14.54\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.977, WER=98.40, CER=62.16\n",
      "\u001b[1m\n",
      "Epoch 39\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.145, WER=48.92, CER=14.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.982, WER=100.98, CER=61.96\n",
      "\u001b[1m\n",
      "Epoch 40\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.139, WER=46.72, CER=13.43\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.005, WER=99.29, CER=62.98\n",
      "\u001b[1m\n",
      "Epoch 41\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.132, WER=45.77, CER=12.76\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.013, WER=98.66, CER=63.20\n",
      "\u001b[1m\n",
      "Epoch 42\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.127, WER=44.17, CER=12.31\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.022, WER=99.04, CER=63.12\n",
      "\u001b[1m\n",
      "Epoch 43\u001b[0m\n",
      "Running training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [45/45, 0.3 min(s)]: Loss=0.122, WER=43.33, CER=11.97\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.043, WER=98.00, CER=63.15\n",
      "\u001b[1m\n",
      "Epoch 44\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.117, WER=41.72, CER=11.28\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.050, WER=97.78, CER=63.50\n",
      "\u001b[1m\n",
      "Epoch 45\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.111, WER=40.87, CER=10.92\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.060, WER=97.87, CER=63.44\n",
      "\u001b[1m\n",
      "Epoch 46\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.105, WER=38.69, CER=10.22\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.089, WER=99.16, CER=63.24\n",
      "\u001b[1m\n",
      "Epoch 47\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.100, WER=36.98, CER=9.65\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.093, WER=99.23, CER=62.99\n",
      "\u001b[1m\n",
      "Epoch 48\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.097, WER=36.86, CER=9.56 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.100, WER=99.09, CER=62.92\n",
      "\u001b[1m\n",
      "Epoch 49\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.090, WER=34.53, CER=8.58\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.108, WER=99.64, CER=62.93\n",
      "\u001b[1m\n",
      "Epoch 50\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.081, WER=31.24, CER=7.63\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.132, WER=99.16, CER=63.07\n",
      "\u001b[1m\n",
      "Epoch 51\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.076, WER=29.23, CER=6.95\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.157, WER=99.14, CER=63.17\n",
      "\u001b[1m\n",
      "Epoch 52\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.071, WER=27.83, CER=6.61\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.160, WER=99.01, CER=63.27\n",
      "\u001b[1m\n",
      "Epoch 53\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.067, WER=26.36, CER=6.18\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.165, WER=98.82, CER=63.18\n",
      "\u001b[1m\n",
      "Epoch 54\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.064, WER=25.35, CER=5.87\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.190, WER=98.98, CER=63.44\n",
      "\u001b[1m\n",
      "Epoch 55\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.061, WER=23.61, CER=5.43\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.191, WER=99.10, CER=63.12\n",
      "\u001b[1m\n",
      "Epoch 56\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.058, WER=23.57, CER=5.32\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.190, WER=99.85, CER=62.77\n",
      "\u001b[1m\n",
      "Epoch 57\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.056, WER=22.85, CER=5.15\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.192, WER=98.90, CER=62.72\n",
      "\u001b[1m\n",
      "Epoch 58\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.054, WER=21.78, CER=4.87\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.205, WER=99.29, CER=62.84\n",
      "\u001b[1m\n",
      "Epoch 59\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.052, WER=21.12, CER=4.78\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.215, WER=99.71, CER=62.77\n",
      "\u001b[1m\n",
      "Epoch 60\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.049, WER=19.76, CER=4.44\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.233, WER=99.54, CER=62.93\n",
      "\u001b[1m\n",
      "Epoch 61\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.047, WER=19.14, CER=4.17\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.235, WER=99.00, CER=63.14\n",
      "\u001b[1m\n",
      "Epoch 62\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.046, WER=18.50, CER=4.04\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.263, WER=99.14, CER=63.13\n",
      "\u001b[1m\n",
      "Epoch 63\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.044, WER=18.39, CER=3.98\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.266, WER=98.40, CER=63.25\n",
      "\u001b[1m\n",
      "Epoch 64\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.043, WER=17.12, CER=3.73\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.280, WER=98.70, CER=63.26\n",
      "\u001b[1m\n",
      "Epoch 65\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.041, WER=17.00, CER=3.63\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.297, WER=98.62, CER=63.63\n",
      "\u001b[1m\n",
      "Epoch 66\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.040, WER=16.36, CER=3.49\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.304, WER=98.23, CER=63.88\n",
      "\u001b[1m\n",
      "Epoch 67\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.038, WER=15.74, CER=3.35\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.332, WER=98.81, CER=64.26\n",
      "\u001b[1m\n",
      "Epoch 68\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.036, WER=15.46, CER=3.22\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.311, WER=98.52, CER=63.41\n",
      "\u001b[1m\n",
      "Epoch 69\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.035, WER=14.39, CER=3.01\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.326, WER=98.18, CER=63.13\n",
      "\u001b[1m\n",
      "Epoch 70\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.033, WER=13.34, CER=2.81\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.339, WER=98.15, CER=63.64\n",
      "\u001b[1m\n",
      "Epoch 71\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.032, WER=13.25, CER=2.72\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.350, WER=98.58, CER=63.66\n",
      "\u001b[1m\n",
      "Epoch 72\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.030, WER=12.65, CER=2.60\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.357, WER=98.24, CER=63.60\n",
      "\u001b[1m\n",
      "Epoch 73\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.029, WER=11.99, CER=2.49\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.368, WER=99.18, CER=63.43\n",
      "\u001b[1m\n",
      "Epoch 74\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.027, WER=10.99, CER=2.29\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.382, WER=99.21, CER=63.23\n",
      "\u001b[1m\n",
      "Epoch 75\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.025, WER=10.11, CER=2.10\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.396, WER=99.42, CER=63.18\n",
      "\u001b[1m\n",
      "Epoch 76\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.024, WER=9.57, CER=1.93 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.402, WER=98.41, CER=63.29\n",
      "\u001b[1m\n",
      "Epoch 77\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.023, WER=9.12, CER=1.84\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.421, WER=98.87, CER=63.62\n",
      "\u001b[1m\n",
      "Epoch 78\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.022, WER=8.45, CER=1.72\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.418, WER=98.35, CER=63.72\n",
      "\u001b[1m\n",
      "Epoch 79\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.021, WER=8.47, CER=1.72\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.438, WER=98.27, CER=63.66\n",
      "\u001b[1m\n",
      "Epoch 80\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.020, WER=7.59, CER=1.53\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.447, WER=98.93, CER=63.23\n",
      "\u001b[1m\n",
      "Epoch 81\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.019, WER=7.31, CER=1.49\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.448, WER=98.25, CER=63.22\n",
      "\u001b[1m\n",
      "Epoch 82\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.019, WER=7.57, CER=1.52\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.463, WER=98.71, CER=63.39\n",
      "\u001b[1m\n",
      "Epoch 83\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.019, WER=6.97, CER=1.41\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.457, WER=99.11, CER=63.34\n",
      "\u001b[1m\n",
      "Epoch 84\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.019, WER=7.32, CER=1.48\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.473, WER=98.51, CER=63.29\n",
      "\u001b[1m\n",
      "Epoch 85\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.018, WER=6.58, CER=1.32\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.474, WER=98.73, CER=63.42\n",
      "\u001b[1m\n",
      "Epoch 86\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.017, WER=6.40, CER=1.28\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.465, WER=97.91, CER=63.46\n",
      "\u001b[1m\n",
      "Epoch 87\u001b[0m\n",
      "Running training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [45/45, 0.3 min(s)]: Loss=0.017, WER=6.50, CER=1.29\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.486, WER=98.04, CER=63.44\n",
      "\u001b[1m\n",
      "Epoch 88\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.016, WER=6.42, CER=1.27\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.473, WER=98.25, CER=63.07\n",
      "\u001b[1m\n",
      "Epoch 89\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.016, WER=6.06, CER=1.17\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.484, WER=98.88, CER=63.38\n",
      "\u001b[1m\n",
      "Epoch 90\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.015, WER=5.80, CER=1.14\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.500, WER=99.49, CER=62.75\n",
      "\u001b[1m\n",
      "Epoch 91\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.015, WER=5.66, CER=1.11\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.495, WER=99.32, CER=62.66\n",
      "\u001b[1m\n",
      "Epoch 92\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.014, WER=5.28, CER=1.04\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.514, WER=98.92, CER=62.70\n",
      "\u001b[1m\n",
      "Epoch 93\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.014, WER=4.80, CER=0.96\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.522, WER=98.86, CER=63.22\n",
      "\u001b[1m\n",
      "Epoch 94\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.013, WER=4.95, CER=0.97\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.518, WER=99.80, CER=62.82\n",
      "\u001b[1m\n",
      "Epoch 95\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.013, WER=5.23, CER=1.02\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.516, WER=100.16, CER=62.98\n",
      "\u001b[1m\n",
      "Epoch 96\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.013, WER=4.80, CER=0.95\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.508, WER=99.23, CER=63.15\n",
      "\u001b[1m\n",
      "Epoch 97\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.012, WER=4.56, CER=0.89\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.534, WER=99.50, CER=63.57\n",
      "\u001b[1m\n",
      "Epoch 98\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.012, WER=4.15, CER=0.81\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.541, WER=98.35, CER=63.46\n",
      "\u001b[1m\n",
      "Epoch 99\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.011, WER=4.19, CER=0.82\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.525, WER=99.14, CER=63.71\n",
      "\u001b[1m\n",
      "Epoch 100\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.011, WER=3.98, CER=0.77\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.555, WER=101.08, CER=63.64\n",
      "\u001b[1m\n",
      "Epoch 101\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.011, WER=3.90, CER=0.73\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.560, WER=99.52, CER=63.52\n",
      "\u001b[1m\n",
      "Epoch 102\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.010, WER=3.66, CER=0.71\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.559, WER=100.14, CER=63.56\n",
      "\u001b[1m\n",
      "Epoch 103\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.010, WER=3.40, CER=0.64\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.582, WER=101.06, CER=63.65\n",
      "\u001b[1m\n",
      "Epoch 104\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.009, WER=3.06, CER=0.58\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.577, WER=101.54, CER=63.26\n",
      "\u001b[1m\n",
      "Epoch 105\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.009, WER=3.33, CER=0.62\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.602, WER=100.94, CER=62.93\n",
      "\u001b[1m\n",
      "Epoch 106\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.009, WER=3.04, CER=0.55\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.612, WER=103.51, CER=63.33\n",
      "\u001b[1m\n",
      "Epoch 107\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.009, WER=3.21, CER=0.60\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.628, WER=103.12, CER=63.26\n",
      "\u001b[1m\n",
      "Epoch 108\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.009, WER=3.12, CER=0.60\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.629, WER=103.41, CER=63.72\n",
      "\u001b[1m\n",
      "Epoch 109\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.009, WER=3.23, CER=0.62\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.651, WER=105.16, CER=64.04\n",
      "\u001b[1m\n",
      "Epoch 110\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.009, WER=3.30, CER=0.59\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.658, WER=105.42, CER=63.43\n",
      "\u001b[1m\n",
      "Epoch 111\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.009, WER=3.29, CER=0.60\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.637, WER=104.29, CER=63.71\n",
      "\u001b[1m\n",
      "Epoch 112\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.008, WER=2.95, CER=0.54\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.667, WER=105.10, CER=64.12\n",
      "\u001b[1m\n",
      "Epoch 113\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.008, WER=2.73, CER=0.50\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.639, WER=104.40, CER=63.42\n",
      "\u001b[1m\n",
      "Epoch 114\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.008, WER=2.71, CER=0.52\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.664, WER=106.36, CER=63.87\n",
      "\u001b[1m\n",
      "Epoch 115\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.008, WER=2.80, CER=0.51\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.669, WER=104.29, CER=64.14\n",
      "\u001b[1m\n",
      "Epoch 116\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.008, WER=2.47, CER=0.47\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.648, WER=101.43, CER=63.46\n",
      "\u001b[1m\n",
      "Epoch 117\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.007, WER=2.09, CER=0.40\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.653, WER=101.67, CER=63.46\n",
      "\u001b[1m\n",
      "Epoch 118\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.007, WER=2.23, CER=0.42\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.664, WER=100.74, CER=63.27\n",
      "\u001b[1m\n",
      "Epoch 119\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.006, WER=1.93, CER=0.36\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.671, WER=101.64, CER=63.35\n",
      "\u001b[1m\n",
      "Epoch 120\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.006, WER=2.12, CER=0.40\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.671, WER=101.20, CER=63.38\n",
      "\u001b[1m\n",
      "Epoch 121\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.006, WER=2.05, CER=0.38\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.671, WER=100.20, CER=63.00\n",
      "\u001b[1m\n",
      "Epoch 122\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.006, WER=1.56, CER=0.29\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.693, WER=99.56, CER=63.27\n",
      "\u001b[1m\n",
      "Epoch 123\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.006, WER=1.65, CER=0.31\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.711, WER=99.31, CER=63.52\n",
      "\u001b[1m\n",
      "Epoch 124\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.005, WER=1.32, CER=0.25\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.700, WER=98.61, CER=62.98\n",
      "\u001b[1m\n",
      "Epoch 125\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.005, WER=1.48, CER=0.29\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.711, WER=98.15, CER=63.08\n",
      "\u001b[1m\n",
      "Epoch 126\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.005, WER=1.38, CER=0.25\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.721, WER=98.99, CER=63.11\n",
      "\u001b[1m\n",
      "Epoch 127\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.005, WER=1.16, CER=0.22\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.741, WER=98.64, CER=63.06\n",
      "\u001b[1m\n",
      "Epoch 128\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.2 min(s)]: Loss=0.004, WER=1.01, CER=0.19\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.739, WER=99.49, CER=62.99\n",
      "\u001b[1m\n",
      "Epoch 129\u001b[0m\n",
      "Running training:\n",
      "Training [45/45, 0.3 min(s)]: Loss=0.004, WER=0.79, CER=0.16\n",
      "Running evaluation:\n",
      "Validation [102/169, 0.7 min(s)]: Loss=1.731, WER=99.11, CER=63.37\r"
     ]
    }
   ],
   "source": [
    "######################################### ASR not-pretained main script #################################################\n",
    "\n",
    "# Import general libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from datetime import datetime\n",
    "\n",
    "import editdistance\n",
    "\n",
    "# Import stuff from other ASR modules\n",
    "from asr.data import BaseDataset\n",
    "from asr.data.preprocessors import SpectrogramPreprocessor, TextPreprocessor\n",
    "from asr.modules import ASRModel\n",
    "from asr.utils.training import batch_to_tensor, epochs, Logger\n",
    "from asr.utils.text import greedy_ctc\n",
    "from asr.utils.metrics import ErrorRateTracker, LossTracker\n",
    "\n",
    "def get_Stats(ref_batch,hyp_batch):\n",
    "    ref_WER_splits = [rf.split() for rf in ref_batch]\n",
    "    hyp_WER_splits = [hp.split() for hp in hyp_batch]\n",
    "    ref_WER_lens = [len(rf_spl) for rf_spl in ref_WER_splits]\n",
    "    hyp_WER_lens = [len(hp_spl) for hp_spl in hyp_WER_splits]\n",
    "    \n",
    "    WER = [editdistance.eval(rf,hp) for rf, hp in zip(ref_WER_splits, hyp_WER_splits)]\n",
    "    \n",
    "    ref_CER_splits = [list(rf) for rf in ref_batch]\n",
    "    hyp_CER_splits = [list(hp) for hp in hyp_batch]\n",
    "    ref_CER_lens = [len(rf_spl) for rf_spl in ref_CER_splits]\n",
    "    hyp_CER_lens = [len(hp_spl) for hp_spl in hyp_CER_splits]\n",
    "    \n",
    "    CER = [editdistance.eval(rf,hp) for rf, hp in zip(ref_CER_splits, hyp_CER_splits)]\n",
    "    \n",
    "    return WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens\n",
    "\n",
    "\n",
    "\"\"\" Function: Train and test ASR model on input datasets\n",
    "    Input:    2 txt-files with IDs to training and test files. One observation consists of a transcript\n",
    "              (txt-feature) and audio file (wav-target).\n",
    "    Output:   Return best WER (validation) and save ASR model (model.pt) \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Part 1: Load and preprocess data \"\"\"\n",
    "train_IDs = 'tacotron2-cut'\n",
    "test_IDs = 'dev-clean'\n",
    "\n",
    "\n",
    "train_source = train_IDs\n",
    "val_source = test_IDs\n",
    "\n",
    "# BLACK BOX\n",
    "spec_preprocessor = SpectrogramPreprocessor(output_format='NFT', sample_rate=4000)\n",
    "text_preprocessor = TextPreprocessor()\n",
    "preprocessor = [spec_preprocessor, text_preprocessor]\n",
    "\n",
    "train_dataset = BaseDataset(source=train_source, preprocessor=preprocessor, sort_by=0)\n",
    "val_dataset = BaseDataset(source=val_source, preprocessor=preprocessor, sort_by=0)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(train_dataset, num_workers=4, pin_memory=True, collate_fn=train_dataset.collate, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, num_workers=4, pin_memory=True, collate_fn=val_dataset.collate, batch_size=16)\n",
    "\n",
    "\"\"\" Part 2: Setup model and loss \"\"\"\n",
    "# Create instance of model\n",
    "asr_model = ASRModel(input_size=40).cuda()\n",
    "print(asr_model)\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in asr_model.parameters() if p.requires_grad))\n",
    "\n",
    "# Define loss, optimizer and learning rate scheduler\n",
    "ctc_loss = nn.CTCLoss(reduction='none').cuda()#reduction='sum'\n",
    "optimizer = torch.optim.Adam(asr_model.parameters(), lr=3e-4)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=5e-5)\n",
    "\n",
    "\"\"\" Part 3: Train and evaluate model \"\"\"\n",
    "# Variables to create and store performance metrics\n",
    "wer_metric = ErrorRateTracker(word_based=True)\n",
    "cer_metric = ErrorRateTracker(word_based=False)\n",
    "ctc_metric = LossTracker()\n",
    "\n",
    "train_logger = Logger('Training', ctc_metric, wer_metric, cer_metric)\n",
    "val_logger = Logger('Validation', ctc_metric, wer_metric, cer_metric)\n",
    "\n",
    "def forward_pass(batch, training=False):\n",
    "    (x, x_sl), (y, y_sl) = batch_to_tensor(batch)  # For CPU: change 'cuda' to 'cpu'\n",
    "    \n",
    "    logits, output_sl = asr_model.forward(x, x_sl.cpu())\n",
    "    log_probs = F.log_softmax(logits, dim=2)\n",
    "    loss = ctc_loss(log_probs, y, output_sl, y_sl)\n",
    "    \n",
    "    hyp_encoded_batch = greedy_ctc(logits, output_sl)\n",
    "    hyp_batch = text_preprocessor.decode_batch(hyp_encoded_batch)\n",
    "    ref_batch = text_preprocessor.decode_batch(y, y_sl)\n",
    "    \n",
    "    wer_metric.update(ref_batch, hyp_batch)\n",
    "    cer_metric.update(ref_batch, hyp_batch)\n",
    "    ctc_metric.update(loss.sum().item(), weight=output_sl.sum().item())\n",
    "\n",
    "    if not training:\n",
    "        WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens = get_Stats(ref_batch,hyp_batch)\n",
    "        CTC_loss = [ctc.item() for ctc in loss]\n",
    "        return CTC_loss, WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "# Run 200 epochs\n",
    "model_name = f\"{train_IDs}vs{test_IDs}{datetime.now().strftime('Y%Y-m%m-d%d-H%H-M%M')}\" \n",
    "with open(f\"./results/{model_name}.csv\", \"a+\") as o_f:\n",
    "    for epoch in epochs(200):\n",
    "        # Set PyTorch in training mode\n",
    "        asr_model.train()\n",
    "\n",
    "        # Train model on training set\n",
    "        print(\"Running training:\")\n",
    "        for batch, files in train_logger(train_loader):\n",
    "            loss = forward_pass(batch, training=True)\n",
    "            optimizer.zero_grad()\n",
    "            loss.sum().backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Set PyTorch in test mode\n",
    "        asr_model.eval()\n",
    "\n",
    "        # Test model using test set\n",
    "        print(\"Running evaluation:\")\n",
    "        N = 1\n",
    "        wer_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, files in val_logger(val_loader):\n",
    "                CTC_loss, WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens = forward_pass(batch, training=False)\n",
    "                wer_sum += np.sum(WER)\n",
    "                N += len(WER)\n",
    "                for i, flac_file in enumerate(files):\n",
    "                    idx = os.path.splitext(os.path.basename(flac_file))[0]\n",
    "                    print(f\"{epoch}\\t{idx}\\t{CTC_loss[i]}\\t{WER[i]}\\t{ref_WER_lens[i]}\\t{hyp_WER_lens[i]}\\t{CER[i]}\\t{ref_CER_lens[i]}\\t{hyp_CER_lens[i]}\", file=o_f)\n",
    "\n",
    "        best_wer = np.inf\n",
    "        if wer_sum/N < best_wer:\n",
    "            best_wer = wer_sum/N\n",
    "            torch.save(asr_model.state_dict(), f\"./results/best_{model_name}.pt\")\n",
    "\n",
    "        if epoch >= 100:\n",
    "            lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-ultimate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

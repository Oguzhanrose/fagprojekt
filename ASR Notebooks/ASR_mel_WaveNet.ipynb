{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "located-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  8 14:05:18 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Tesla V1...  On   | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    55W / 300W |   3492MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA Tesla V1...  On   | 00000000:16:00.0 Off |                    0 |\n",
      "| N/A   43C    P0   157W / 300W |  26163MiB / 32510MiB |     64%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA Tesla V1...  On   | 00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   53C    P0   308W / 300W |  31334MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA Tesla V1...  On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   65C    P0   298W / 300W |  26652MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     12379      C   ...995_torch_gpu/bin/python3     3489MiB |\n",
      "|    1   N/A  N/A     13332      C   .../envs/kerasgpu/bin/python    24511MiB |\n",
      "|    1   N/A  N/A     22535      C   python                           1647MiB |\n",
      "|    2   N/A  N/A      5880      C   ...python/3.6.13/bin/python3    31331MiB |\n",
      "|    3   N/A  N/A      5880      C   ...python/3.6.13/bin/python3    26649MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-effort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASRModel(\n",
      "  (conv1d_layer): Conv1d(40, 256, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "  (lstm_block): LSTM(256, 256, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (output_layer): Linear(in_features=512, out_features=29, bias=True)\n",
      ")\n",
      "Trainable parameters: 2695965\n",
      "\u001b[1m\n",
      "Epoch 1\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=1.125, WER=99.99, CER=99.98 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.855, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 2\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.854, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.854, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 3\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.852, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.854, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 4\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.851, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.854, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 5\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.850, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.854, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 6\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.850, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.1 min(s)]: Loss=0.853, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 7\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.849, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.853, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 8\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.848, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.850, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 9\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.846, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.3 min(s)]: Loss=0.846, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 10\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.842, WER=100.00, CER=99.39 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.838, WER=100.00, CER=91.77\n",
      "\u001b[1m\n",
      "Epoch 11\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.835, WER=100.00, CER=92.93\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.825, WER=100.00, CER=85.35\n",
      "\u001b[1m\n",
      "Epoch 12\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.825, WER=100.00, CER=89.19\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.812, WER=100.00, CER=82.96\n",
      "\u001b[1m\n",
      "Epoch 13\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.815, WER=100.00, CER=85.80\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.807, WER=100.00, CER=82.41\n",
      "\u001b[1m\n",
      "Epoch 14\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.806, WER=100.00, CER=83.46\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.803, WER=100.00, CER=82.42\n",
      "\u001b[1m\n",
      "Epoch 15\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.802, WER=100.00, CER=83.51\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.800, WER=100.00, CER=82.77\n",
      "\u001b[1m\n",
      "Epoch 16\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.799, WER=99.97, CER=83.25 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.800, WER=100.00, CER=83.78\n",
      "\u001b[1m\n",
      "Epoch 17\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.799, WER=98.59, CER=80.35\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.1 min(s)]: Loss=0.798, WER=99.25, CER=81.30\n",
      "\u001b[1m\n",
      "Epoch 18\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.796, WER=98.34, CER=80.01\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.792, WER=98.00, CER=79.23\n",
      "\u001b[1m\n",
      "Epoch 19\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.790, WER=100.88, CER=74.99\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.791, WER=98.98, CER=76.58\n",
      "\u001b[1m\n",
      "Epoch 20\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.789, WER=99.26, CER=77.82\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.786, WER=100.59, CER=75.60\n",
      "\u001b[1m\n",
      "Epoch 21\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.784, WER=102.34, CER=74.08\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.785, WER=100.90, CER=75.16\n",
      "\u001b[1m\n",
      "Epoch 22\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.786, WER=100.82, CER=76.03\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.790, WER=99.57, CER=80.14\n",
      "\u001b[1m\n",
      "Epoch 23\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.794, WER=98.85, CER=80.66\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.785, WER=99.20, CER=78.73\n",
      "\u001b[1m\n",
      "Epoch 24\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.786, WER=98.55, CER=78.13\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.781, WER=99.20, CER=76.30\n",
      "\u001b[1m\n",
      "Epoch 25\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.781, WER=99.29, CER=75.73\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.777, WER=100.63, CER=74.67\n",
      "\u001b[1m\n",
      "Epoch 26\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.779, WER=99.20, CER=75.74\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.776, WER=100.99, CER=74.80\n",
      "\u001b[1m\n",
      "Epoch 27\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.776, WER=100.07, CER=74.83\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.1 min(s)]: Loss=0.774, WER=99.72, CER=75.09\n",
      "\u001b[1m\n",
      "Epoch 28\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.774, WER=100.13, CER=74.53\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.773, WER=101.53, CER=74.47\n",
      "\u001b[1m\n",
      "Epoch 29\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.773, WER=101.48, CER=74.23\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.771, WER=102.45, CER=73.75\n",
      "\u001b[1m\n",
      "Epoch 30\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.771, WER=103.27, CER=73.65\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.771, WER=104.18, CER=73.14\n",
      "\u001b[1m\n",
      "Epoch 31\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.772, WER=102.79, CER=73.81\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.770, WER=105.66, CER=72.77\n",
      "\u001b[1m\n",
      "Epoch 32\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.769, WER=104.69, CER=73.42\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.769, WER=104.18, CER=72.65\n",
      "\u001b[1m\n",
      "Epoch 33\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.770, WER=103.74, CER=73.76\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.768, WER=103.93, CER=72.58\n",
      "\u001b[1m\n",
      "Epoch 34\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.772, WER=102.43, CER=74.30\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.1 min(s)]: Loss=0.767, WER=105.05, CER=72.50\n",
      "\u001b[1m\n",
      "Epoch 35\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.768, WER=103.79, CER=73.56\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=105.86, CER=71.70\n",
      "\u001b[1m\n",
      "Epoch 36\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.767, WER=105.66, CER=72.92\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=106.53, CER=72.16\n",
      "\u001b[1m\n",
      "Epoch 37\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.766, WER=107.05, CER=72.92\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.766, WER=108.95, CER=71.84\n",
      "\u001b[1m\n",
      "Epoch 38\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.766, WER=106.55, CER=73.23\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=112.01, CER=71.35\n",
      "\u001b[1m\n",
      "Epoch 39\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.767, WER=106.11, CER=73.20\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=109.21, CER=71.61\n",
      "\u001b[1m\n",
      "Epoch 40\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.768, WER=105.66, CER=73.29\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.1 min(s)]: Loss=0.766, WER=115.84, CER=71.69\n",
      "\u001b[1m\n",
      "Epoch 41\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.766, WER=108.56, CER=73.07\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.766, WER=112.14, CER=72.09\n",
      "\u001b[1m\n",
      "Epoch 42\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.763, WER=111.03, CER=72.58\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=113.90, CER=71.37\n",
      "\u001b[1m\n",
      "Epoch 43\u001b[0m\n",
      "Running training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [51/51, 0.2 min(s)]: Loss=0.764, WER=109.56, CER=72.86\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=107.80, CER=72.12\n",
      "\u001b[1m\n",
      "Epoch 44\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.766, WER=105.16, CER=73.68\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=115.01, CER=70.86\n",
      "\u001b[1m\n",
      "Epoch 45\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.763, WER=108.45, CER=72.91\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=113.33, CER=71.27\n",
      "\u001b[1m\n",
      "Epoch 46\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.761, WER=111.93, CER=72.66\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.1 min(s)]: Loss=0.764, WER=116.84, CER=71.39\n",
      "\u001b[1m\n",
      "Epoch 47\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.761, WER=109.31, CER=72.81\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=114.05, CER=70.91\n",
      "\u001b[1m\n",
      "Epoch 48\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.762, WER=108.85, CER=72.90\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=113.11, CER=71.23\n",
      "\u001b[1m\n",
      "Epoch 49\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.767, WER=102.83, CER=74.78\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=109.12, CER=71.44\n",
      "\u001b[1m\n",
      "Epoch 50\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.760, WER=108.30, CER=72.86\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=119.16, CER=70.71\n",
      "\u001b[1m\n",
      "Epoch 51\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.760, WER=112.35, CER=72.55\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.1 min(s)]: Loss=0.762, WER=118.53, CER=70.89\n",
      "\u001b[1m\n",
      "Epoch 52\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.763, WER=108.45, CER=73.45\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.768, WER=103.92, CER=73.08\n",
      "\u001b[1m\n",
      "Epoch 53\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.771, WER=101.01, CER=76.39\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.766, WER=105.24, CER=72.16\n",
      "\u001b[1m\n",
      "Epoch 54\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.767, WER=101.23, CER=74.79\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=106.12, CER=72.06\n",
      "\u001b[1m\n",
      "Epoch 55\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.765, WER=101.86, CER=73.95\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=108.47, CER=71.70\n",
      "\u001b[1m\n",
      "Epoch 56\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.762, WER=103.73, CER=73.76\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=110.12, CER=71.40\n",
      "\u001b[1m\n",
      "Epoch 57\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.761, WER=104.09, CER=73.49\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=108.97, CER=71.66\n",
      "\u001b[1m\n",
      "Epoch 58\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.760, WER=105.77, CER=72.88\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=110.30, CER=71.45\n",
      "\u001b[1m\n",
      "Epoch 59\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.759, WER=106.65, CER=72.54\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=109.36, CER=71.67\n",
      "\u001b[1m\n",
      "Epoch 60\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.758, WER=106.60, CER=72.62\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=112.23, CER=71.26\n",
      "\u001b[1m\n",
      "Epoch 61\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.758, WER=106.21, CER=72.59\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=113.09, CER=71.09\n",
      "\u001b[1m\n",
      "Epoch 62\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.757, WER=106.83, CER=72.48\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=113.61, CER=71.02\n",
      "\u001b[1m\n",
      "Epoch 63\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.758, WER=105.57, CER=72.54\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=109.06, CER=71.51\n",
      "\u001b[1m\n",
      "Epoch 64\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.758, WER=106.31, CER=72.57\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=113.11, CER=71.14\n",
      "\u001b[1m\n",
      "Epoch 65\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.760, WER=107.26, CER=72.36\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=109.72, CER=71.57\n",
      "\u001b[1m\n",
      "Epoch 66\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.758, WER=106.14, CER=72.67\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=113.03, CER=71.10\n",
      "\u001b[1m\n",
      "Epoch 67\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.756, WER=108.21, CER=72.10\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=113.51, CER=71.12\n",
      "\u001b[1m\n",
      "Epoch 68\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.756, WER=108.28, CER=72.08\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=109.66, CER=71.44\n",
      "\u001b[1m\n",
      "Epoch 69\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.757, WER=107.93, CER=72.11\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=113.99, CER=70.99\n",
      "\u001b[1m\n",
      "Epoch 70\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.755, WER=108.77, CER=72.20\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=109.84, CER=71.47\n",
      "\u001b[1m\n",
      "Epoch 71\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.757, WER=105.51, CER=72.82\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=107.10, CER=71.76\n",
      "\u001b[1m\n",
      "Epoch 72\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.756, WER=105.06, CER=72.95\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=112.92, CER=70.99\n",
      "\u001b[1m\n",
      "Epoch 73\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.753, WER=108.60, CER=72.09\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=111.42, CER=71.36\n",
      "\u001b[1m\n",
      "Epoch 74\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.754, WER=107.74, CER=72.16\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=110.19, CER=71.56\n",
      "\u001b[1m\n",
      "Epoch 75\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.751, WER=107.24, CER=72.13\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=111.67, CER=71.17\n",
      "\u001b[1m\n",
      "Epoch 76\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.752, WER=107.72, CER=71.91\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=110.82, CER=71.40\n",
      "\u001b[1m\n",
      "Epoch 77\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.752, WER=107.12, CER=72.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=112.09, CER=71.13\n",
      "\u001b[1m\n",
      "Epoch 78\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.751, WER=107.49, CER=71.89\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.762, WER=112.33, CER=71.02\n",
      "\u001b[1m\n",
      "Epoch 79\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.750, WER=106.84, CER=71.97\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.1 min(s)]: Loss=0.763, WER=111.62, CER=71.19\n",
      "\u001b[1m\n",
      "Epoch 80\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.751, WER=106.22, CER=72.31\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=111.33, CER=71.06\n",
      "\u001b[1m\n",
      "Epoch 81\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.755, WER=103.79, CER=72.97\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=109.72, CER=71.70\n",
      "\u001b[1m\n",
      "Epoch 82\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.753, WER=108.10, CER=71.90\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=110.25, CER=71.44\n",
      "\u001b[1m\n",
      "Epoch 83\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.753, WER=107.35, CER=71.85\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=110.48, CER=71.48\n",
      "\u001b[1m\n",
      "Epoch 84\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.752, WER=105.85, CER=72.04\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=110.34, CER=71.34\n",
      "\u001b[1m\n",
      "Epoch 85\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.749, WER=106.64, CER=71.92\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=108.91, CER=71.71\n",
      "\u001b[1m\n",
      "Epoch 86\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.747, WER=107.68, CER=71.69\n",
      "Running evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=113.14, CER=71.08\n",
      "\u001b[1m\n",
      "Epoch 87\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.746, WER=108.09, CER=71.52\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=111.47, CER=71.21\n",
      "\u001b[1m\n",
      "Epoch 88\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.749, WER=106.12, CER=71.87\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.766, WER=110.07, CER=71.50\n",
      "\u001b[1m\n",
      "Epoch 89\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.747, WER=107.98, CER=71.40\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=108.33, CER=71.99\n",
      "\u001b[1m\n",
      "Epoch 90\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.745, WER=107.65, CER=71.33\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=112.01, CER=71.21\n",
      "\u001b[1m\n",
      "Epoch 91\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.745, WER=107.93, CER=71.26\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=110.36, CER=71.51\n",
      "\u001b[1m\n",
      "Epoch 92\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.753, WER=108.51, CER=71.30\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.770, WER=108.95, CER=71.83\n",
      "\u001b[1m\n",
      "Epoch 93\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.747, WER=108.91, CER=71.31\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=112.21, CER=71.28\n",
      "\u001b[1m\n",
      "Epoch 94\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.752, WER=110.71, CER=71.28\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=109.87, CER=71.76\n",
      "\u001b[1m\n",
      "Epoch 95\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.749, WER=110.48, CER=71.24\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=113.93, CER=71.04\n",
      "\u001b[1m\n",
      "Epoch 96\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.752, WER=109.46, CER=71.53\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.768, WER=123.99, CER=70.07\n",
      "\u001b[1m\n",
      "Epoch 97\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.753, WER=113.18, CER=71.28\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.764, WER=117.79, CER=70.76\n",
      "\u001b[1m\n",
      "Epoch 98\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.751, WER=113.69, CER=71.20\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.766, WER=113.45, CER=71.09\n",
      "\u001b[1m\n",
      "Epoch 99\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.752, WER=111.24, CER=71.44\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.763, WER=120.34, CER=70.29\n",
      "\u001b[1m\n",
      "Epoch 100\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.750, WER=112.75, CER=71.18\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.773, WER=107.78, CER=72.52\n",
      "\u001b[1m\n",
      "Epoch 101\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.754, WER=112.33, CER=71.26\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.766, WER=113.66, CER=71.29\n",
      "\u001b[1m\n",
      "Epoch 102\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.750, WER=112.19, CER=71.28\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.765, WER=116.70, CER=70.69\n",
      "\u001b[1m\n",
      "Epoch 103\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.757, WER=107.12, CER=72.61\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.773, WER=104.10, CER=72.62\n",
      "\u001b[1m\n",
      "Epoch 104\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.762, WER=100.76, CER=74.18\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.770, WER=102.59, CER=73.10\n",
      "\u001b[1m\n",
      "Epoch 105\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.755, WER=102.31, CER=72.97\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=107.63, CER=71.96\n",
      "\u001b[1m\n",
      "Epoch 106\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.750, WER=109.75, CER=71.24\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.1 min(s)]: Loss=0.765, WER=113.52, CER=70.99\n",
      "\u001b[1m\n",
      "Epoch 107\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.756, WER=107.22, CER=72.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.773, WER=100.94, CER=74.35\n",
      "\u001b[1m\n",
      "Epoch 108\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.759, WER=106.50, CER=72.35\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.770, WER=108.91, CER=72.12\n",
      "\u001b[1m\n",
      "Epoch 109\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.755, WER=106.18, CER=71.97\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.768, WER=106.90, CER=72.09\n",
      "\u001b[1m\n",
      "Epoch 110\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.757, WER=107.03, CER=71.96\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.769, WER=106.28, CER=72.81\n",
      "\u001b[1m\n",
      "Epoch 111\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.754, WER=107.11, CER=71.93\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.779, WER=104.55, CER=73.33\n",
      "\u001b[1m\n",
      "Epoch 112\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.757, WER=104.42, CER=72.13\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.1 min(s)]: Loss=0.767, WER=108.86, CER=71.80\n",
      "\u001b[1m\n",
      "Epoch 113\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.753, WER=105.71, CER=71.85\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.768, WER=105.97, CER=72.22\n",
      "\u001b[1m\n",
      "Epoch 114\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.752, WER=106.11, CER=71.83\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=108.39, CER=71.99\n",
      "\u001b[1m\n",
      "Epoch 115\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.750, WER=108.62, CER=71.59\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.767, WER=112.43, CER=71.15\n",
      "\u001b[1m\n",
      "Epoch 116\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.750, WER=109.90, CER=71.44\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.768, WER=107.48, CER=72.22\n",
      "\u001b[1m\n",
      "Epoch 117\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.748, WER=110.02, CER=71.29\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.772, WER=104.31, CER=73.14\n",
      "\u001b[1m\n",
      "Epoch 118\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.747, WER=110.42, CER=71.19\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.774, WER=105.17, CER=72.76\n",
      "\u001b[1m\n",
      "Epoch 119\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.747, WER=110.52, CER=71.07\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.774, WER=108.64, CER=71.97\n",
      "\u001b[1m\n",
      "Epoch 120\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.747, WER=111.10, CER=71.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.774, WER=107.54, CER=72.21\n",
      "\u001b[1m\n",
      "Epoch 121\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.747, WER=109.40, CER=71.18\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.774, WER=109.28, CER=71.63\n",
      "\u001b[1m\n",
      "Epoch 122\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.755, WER=108.33, CER=71.55\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.774, WER=105.66, CER=72.63\n",
      "\u001b[1m\n",
      "Epoch 123\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.751, WER=107.79, CER=71.46\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.774, WER=106.01, CER=72.44\n",
      "\u001b[1m\n",
      "Epoch 124\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.752, WER=109.20, CER=71.36\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.777, WER=105.78, CER=72.71\n",
      "\u001b[1m\n",
      "Epoch 125\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.750, WER=109.75, CER=71.19\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.787, WER=103.20, CER=74.03\n",
      "\u001b[1m\n",
      "Epoch 126\u001b[0m\n",
      "Running training:\n",
      "Training [51/51, 0.2 min(s)]: Loss=0.747, WER=110.20, CER=71.07\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.780, WER=103.53, CER=73.45\n",
      "\u001b[1m\n",
      "Epoch 127\u001b[0m\n",
      "Running training:\n",
      "Training [36/51, 0.1 min(s)]: Loss=0.750, WER=108.96, CER=71.14\r"
     ]
    }
   ],
   "source": [
    "######################################### ASR not-pretained main script #################################################\n",
    "\n",
    "# Import general libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from datetime import datetime\n",
    "\n",
    "import editdistance\n",
    "\n",
    "# Import stuff from other ASR modules\n",
    "from asr.data import BaseDataset\n",
    "from asr.data.preprocessors import SpectrogramPreprocessor, TextPreprocessor\n",
    "from asr.modules import ASRModel\n",
    "from asr.utils.training import batch_to_tensor, epochs, Logger\n",
    "from asr.utils.text import greedy_ctc\n",
    "from asr.utils.metrics import ErrorRateTracker, LossTracker\n",
    "\n",
    "def get_Stats(ref_batch,hyp_batch):\n",
    "    ref_WER_splits = [rf.split() for rf in ref_batch]\n",
    "    hyp_WER_splits = [hp.split() for hp in hyp_batch]\n",
    "    ref_WER_lens = [len(rf_spl) for rf_spl in ref_WER_splits]\n",
    "    hyp_WER_lens = [len(hp_spl) for hp_spl in hyp_WER_splits]\n",
    "    \n",
    "    WER = [editdistance.eval(rf,hp) for rf, hp in zip(ref_WER_splits, hyp_WER_splits)]\n",
    "    \n",
    "    ref_CER_splits = [list(rf) for rf in ref_batch]\n",
    "    hyp_CER_splits = [list(hp) for hp in hyp_batch]\n",
    "    ref_CER_lens = [len(rf_spl) for rf_spl in ref_CER_splits]\n",
    "    hyp_CER_lens = [len(hp_spl) for hp_spl in hyp_CER_splits]\n",
    "    \n",
    "    CER = [editdistance.eval(rf,hp) for rf, hp in zip(ref_CER_splits, hyp_CER_splits)]\n",
    "    \n",
    "    return WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens\n",
    "\n",
    "\n",
    "\"\"\" Function: Train and test ASR model on input datasets\n",
    "    Input:    2 txt-files with IDs to training and test files. One observation consists of a transcript\n",
    "              (txt-feature) and audio file (wav-target).\n",
    "    Output:   Return best WER (validation) and save ASR model (model.pt) \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Part 1: Load and preprocess data \"\"\"\n",
    "train_IDs = 'wavenet-tacotron2'\n",
    "test_IDs = 'dev-clean'\n",
    "\n",
    "\n",
    "train_source = train_IDs\n",
    "val_source = test_IDs\n",
    "\n",
    "# BLACK BOX\n",
    "spec_preprocessor = SpectrogramPreprocessor(output_format='NFT', sample_rate=4000)\n",
    "text_preprocessor = TextPreprocessor()\n",
    "preprocessor = [spec_preprocessor, text_preprocessor]\n",
    "\n",
    "train_dataset = BaseDataset(source=train_source, preprocessor=preprocessor, sort_by=0)\n",
    "val_dataset = BaseDataset(source=val_source, preprocessor=preprocessor, sort_by=0)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(train_dataset, num_workers=4, pin_memory=True, collate_fn=train_dataset.collate, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, num_workers=4, pin_memory=True, collate_fn=val_dataset.collate, batch_size=16)\n",
    "\n",
    "\"\"\" Part 2: Setup model and loss \"\"\"\n",
    "# Create instance of model\n",
    "asr_model = ASRModel(input_size=40).cuda()\n",
    "print(asr_model)\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in asr_model.parameters() if p.requires_grad))\n",
    "\n",
    "# Define loss, optimizer and learning rate scheduler\n",
    "ctc_loss = nn.CTCLoss(reduction='none').cuda()#reduction='sum'\n",
    "optimizer = torch.optim.Adam(asr_model.parameters(), lr=3e-4)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=5e-5)\n",
    "\n",
    "\"\"\" Part 3: Train and evaluate model \"\"\"\n",
    "# Variables to create and store performance metrics\n",
    "wer_metric = ErrorRateTracker(word_based=True)\n",
    "cer_metric = ErrorRateTracker(word_based=False)\n",
    "ctc_metric = LossTracker()\n",
    "\n",
    "train_logger = Logger('Training', ctc_metric, wer_metric, cer_metric)\n",
    "val_logger = Logger('Validation', ctc_metric, wer_metric, cer_metric)\n",
    "\n",
    "def forward_pass(batch, training=False):\n",
    "    (x, x_sl), (y, y_sl) = batch_to_tensor(batch)  # For CPU: change 'cuda' to 'cpu'\n",
    "    \n",
    "    logits, output_sl = asr_model.forward(x, x_sl.cpu())\n",
    "    log_probs = F.log_softmax(logits, dim=2)\n",
    "    loss = ctc_loss(log_probs, y, output_sl, y_sl)\n",
    "    \n",
    "    hyp_encoded_batch = greedy_ctc(logits, output_sl)\n",
    "    hyp_batch = text_preprocessor.decode_batch(hyp_encoded_batch)\n",
    "    ref_batch = text_preprocessor.decode_batch(y, y_sl)\n",
    "    \n",
    "    wer_metric.update(ref_batch, hyp_batch)\n",
    "    cer_metric.update(ref_batch, hyp_batch)\n",
    "    ctc_metric.update(loss.sum().item(), weight=output_sl.sum().item())\n",
    "\n",
    "    if not training:\n",
    "        WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens = get_Stats(ref_batch,hyp_batch)\n",
    "        CTC_loss = [ctc.item() for ctc in loss]\n",
    "        return CTC_loss, WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "# Run 200 epochs\n",
    "model_name = f\"{train_IDs}vs{test_IDs}{datetime.now().strftime('Y%Y-m%m-d%d-H%H-M%M')}\" \n",
    "with open(f\"./results/{model_name}.csv\", \"a+\") as o_f:\n",
    "    for epoch in epochs(200):\n",
    "        # Set PyTorch in training mode\n",
    "        asr_model.train()\n",
    "\n",
    "        # Train model on training set\n",
    "        print(\"Running training:\")\n",
    "        for batch, files in train_logger(train_loader):\n",
    "            loss = forward_pass(batch, training=True)\n",
    "            optimizer.zero_grad()\n",
    "            loss.sum().backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Set PyTorch in test mode\n",
    "        asr_model.eval()\n",
    "\n",
    "        # Test model using test set\n",
    "        print(\"Running evaluation:\")\n",
    "        N = 1\n",
    "        wer_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, files in val_logger(val_loader):\n",
    "                CTC_loss, WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens = forward_pass(batch, training=False)\n",
    "                wer_sum += np.sum(WER)\n",
    "                N += len(WER)\n",
    "                for i, flac_file in enumerate(files):\n",
    "                    idx = os.path.splitext(os.path.basename(flac_file))[0]\n",
    "                    print(f\"{epoch}\\t{idx}\\t{CTC_loss[i]}\\t{WER[i]}\\t{ref_WER_lens[i]}\\t{hyp_WER_lens[i]}\\t{CER[i]}\\t{ref_CER_lens[i]}\\t{hyp_CER_lens[i]}\", file=o_f)\n",
    "\n",
    "        best_wer = np.inf\n",
    "        if wer_sum/N < best_wer:\n",
    "            best_wer = wer_sum/N\n",
    "            torch.save(asr_model.state_dict(), f\"./results/best_{model_name}.pt\")\n",
    "\n",
    "        if epoch >= 100:\n",
    "            lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-territory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

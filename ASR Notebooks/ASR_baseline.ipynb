{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "located-wonder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  7 13:42:38 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Tesla V1...  On   | 00000000:37:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    26W / 250W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA Tesla V1...  On   | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   73C    P0   189W / 250W |  15729MiB / 16160MiB |     92%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A      5642      C   python3                         15725MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-effort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASRModel(\n",
      "  (conv1d_layer): Conv1d(40, 256, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "  (lstm_block): LSTM(256, 256, num_layers=2, dropout=0.4, bidirectional=True)\n",
      "  (output_layer): Linear(in_features=512, out_features=29, bias=True)\n",
      ")\n",
      "Trainable parameters: 2695965\n",
      "\u001b[1m\n",
      "Epoch 1\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=1.070, WER=99.99, CER=100.14\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.857, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 2\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.842, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.853, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 3\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.838, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.848, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 4\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.833, WER=100.00, CER=100.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.841, WER=100.00, CER=100.00\n",
      "\u001b[1m\n",
      "Epoch 5\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.822, WER=100.00, CER=99.13 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.819, WER=100.00, CER=89.66\n",
      "\u001b[1m\n",
      "Epoch 6\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.791, WER=100.00, CER=85.92\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.779, WER=100.00, CER=83.31\n",
      "\u001b[1m\n",
      "Epoch 7\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.752, WER=99.99, CER=80.95 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.741, WER=100.00, CER=79.00\n",
      "\u001b[1m\n",
      "Epoch 8\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.713, WER=99.75, CER=77.14 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.706, WER=99.17, CER=75.59\n",
      "\u001b[1m\n",
      "Epoch 9\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.679, WER=98.69, CER=72.72\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.678, WER=98.42, CER=72.27\n",
      "\u001b[1m\n",
      "Epoch 10\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.648, WER=100.11, CER=68.67\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.653, WER=98.49, CER=69.44\n",
      "\u001b[1m\n",
      "Epoch 11\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.621, WER=101.80, CER=65.86\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.630, WER=99.32, CER=66.50\n",
      "\u001b[1m\n",
      "Epoch 12\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.596, WER=102.32, CER=63.35\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.611, WER=101.84, CER=63.81\n",
      "\u001b[1m\n",
      "Epoch 13\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.573, WER=101.88, CER=61.23\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.595, WER=101.74, CER=61.75\n",
      "\u001b[1m\n",
      "Epoch 14\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.553, WER=101.50, CER=58.93\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.581, WER=100.18, CER=60.12\n",
      "\u001b[1m\n",
      "Epoch 15\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.535, WER=100.73, CER=57.22\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.570, WER=101.22, CER=58.65\n",
      "\u001b[1m\n",
      "Epoch 16\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.517, WER=100.00, CER=55.35\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.559, WER=101.20, CER=57.17\n",
      "\u001b[1m\n",
      "Epoch 17\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.501, WER=99.14, CER=53.56 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.551, WER=99.72, CER=56.17\n",
      "\u001b[1m\n",
      "Epoch 18\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.485, WER=97.88, CER=51.94\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.546, WER=98.62, CER=55.40\n",
      "\u001b[1m\n",
      "Epoch 19\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.471, WER=96.97, CER=50.30\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.539, WER=98.73, CER=54.39\n",
      "\u001b[1m\n",
      "Epoch 20\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.457, WER=96.22, CER=48.83\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.535, WER=98.78, CER=53.71\n",
      "\u001b[1m\n",
      "Epoch 21\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.443, WER=94.98, CER=47.53\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.532, WER=99.42, CER=52.99\n",
      "\u001b[1m\n",
      "Epoch 22\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.430, WER=93.96, CER=46.17\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.531, WER=100.36, CER=52.54\n",
      "\u001b[1m\n",
      "Epoch 23\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.417, WER=92.77, CER=44.80\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.531, WER=101.06, CER=52.21\n",
      "\u001b[1m\n",
      "Epoch 24\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.404, WER=91.91, CER=43.43\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.535, WER=101.73, CER=52.33\n",
      "\u001b[1m\n",
      "Epoch 25\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.392, WER=90.80, CER=42.28\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.536, WER=102.17, CER=52.06\n",
      "\u001b[1m\n",
      "Epoch 26\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.380, WER=89.21, CER=41.00\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.536, WER=102.60, CER=51.78\n",
      "\u001b[1m\n",
      "Epoch 27\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.369, WER=88.63, CER=39.84\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.538, WER=100.37, CER=51.74\n",
      "\u001b[1m\n",
      "Epoch 28\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.357, WER=87.25, CER=38.51\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.541, WER=98.90, CER=51.81\n",
      "\u001b[1m\n",
      "Epoch 29\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.346, WER=86.28, CER=37.48\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.542, WER=98.78, CER=51.34\n",
      "\u001b[1m\n",
      "Epoch 30\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.335, WER=84.80, CER=36.34\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.547, WER=98.31, CER=51.23\n",
      "\u001b[1m\n",
      "Epoch 31\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.326, WER=83.56, CER=35.37\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.548, WER=97.58, CER=50.90\n",
      "\u001b[1m\n",
      "Epoch 32\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.317, WER=82.80, CER=34.41\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.558, WER=97.78, CER=50.98\n",
      "\u001b[1m\n",
      "Epoch 33\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.308, WER=81.48, CER=33.42\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.560, WER=97.04, CER=51.25\n",
      "\u001b[1m\n",
      "Epoch 34\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.298, WER=80.32, CER=32.38\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.566, WER=98.09, CER=51.28\n",
      "\u001b[1m\n",
      "Epoch 35\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.289, WER=79.27, CER=31.37\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.566, WER=96.90, CER=51.19\n",
      "\u001b[1m\n",
      "Epoch 36\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.280, WER=77.93, CER=30.43\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.562, WER=96.89, CER=50.59\n",
      "\u001b[1m\n",
      "Epoch 37\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.269, WER=76.26, CER=29.31\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.570, WER=98.40, CER=50.43\n",
      "\u001b[1m\n",
      "Epoch 38\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.261, WER=75.56, CER=28.43\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.576, WER=98.78, CER=50.43\n",
      "\u001b[1m\n",
      "Epoch 39\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.252, WER=74.25, CER=27.48\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.582, WER=98.66, CER=50.14\n",
      "\u001b[1m\n",
      "Epoch 40\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.244, WER=72.97, CER=26.75\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.581, WER=96.42, CER=49.97\n",
      "\u001b[1m\n",
      "Epoch 41\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.237, WER=71.84, CER=25.86\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.588, WER=94.59, CER=50.18\n",
      "\u001b[1m\n",
      "Epoch 42\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.229, WER=70.54, CER=25.07\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.590, WER=93.79, CER=50.17\n",
      "\u001b[1m\n",
      "Epoch 43\u001b[0m\n",
      "Running training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [63/63, 0.5 min(s)]: Loss=0.223, WER=69.69, CER=24.38\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.597, WER=95.29, CER=50.02\n",
      "\u001b[1m\n",
      "Epoch 44\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.217, WER=69.41, CER=23.90\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.606, WER=96.91, CER=50.46\n",
      "\u001b[1m\n",
      "Epoch 45\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.212, WER=68.64, CER=23.37\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.607, WER=97.60, CER=50.51\n",
      "\u001b[1m\n",
      "Epoch 46\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.208, WER=68.12, CER=23.10\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.626, WER=99.46, CER=51.47\n",
      "\u001b[1m\n",
      "Epoch 47\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.202, WER=66.77, CER=22.38\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.627, WER=99.94, CER=51.01 \n",
      "\u001b[1m\n",
      "Epoch 48\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.194, WER=65.25, CER=21.54\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.628, WER=98.33, CER=50.84\n",
      "\u001b[1m\n",
      "Epoch 49\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.187, WER=64.37, CER=20.78\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.628, WER=95.92, CER=50.74\n",
      "\u001b[1m\n",
      "Epoch 50\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.179, WER=63.10, CER=19.93\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.629, WER=96.42, CER=50.24\n",
      "\u001b[1m\n",
      "Epoch 51\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.171, WER=61.10, CER=18.96\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.638, WER=94.93, CER=50.33\n",
      "\u001b[1m\n",
      "Epoch 52\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.162, WER=59.24, CER=17.96\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.651, WER=94.83, CER=50.47\n",
      "\u001b[1m\n",
      "Epoch 53\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.155, WER=57.39, CER=17.15\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.659, WER=95.44, CER=50.41\n",
      "\u001b[1m\n",
      "Epoch 54\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.148, WER=55.41, CER=16.15\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.666, WER=95.52, CER=50.41\n",
      "\u001b[1m\n",
      "Epoch 55\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.142, WER=54.30, CER=15.58\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.679, WER=96.79, CER=50.65\n",
      "\u001b[1m\n",
      "Epoch 56\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.137, WER=53.27, CER=15.14\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.692, WER=96.91, CER=50.70\n",
      "\u001b[1m\n",
      "Epoch 57\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.131, WER=51.34, CER=14.26\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.694, WER=97.01, CER=50.67\n",
      "\u001b[1m\n",
      "Epoch 58\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.127, WER=50.41, CER=14.01\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.701, WER=96.90, CER=50.68\n",
      "\u001b[1m\n",
      "Epoch 59\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.123, WER=49.17, CER=13.32\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.706, WER=96.91, CER=50.78\n",
      "\u001b[1m\n",
      "Epoch 60\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.118, WER=48.09, CER=12.84\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.712, WER=95.26, CER=50.95\n",
      "\u001b[1m\n",
      "Epoch 61\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.115, WER=47.50, CER=12.71\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.719, WER=94.85, CER=50.66\n",
      "\u001b[1m\n",
      "Epoch 62\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.111, WER=46.73, CER=12.36\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.737, WER=93.91, CER=51.21\n",
      "\u001b[1m\n",
      "Epoch 63\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.109, WER=46.02, CER=12.04\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.748, WER=94.16, CER=51.39\n",
      "\u001b[1m\n",
      "Epoch 64\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.107, WER=45.67, CER=11.97\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.757, WER=94.28, CER=51.43\n",
      "\u001b[1m\n",
      "Epoch 65\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.105, WER=45.42, CER=11.77\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.758, WER=94.17, CER=51.45\n",
      "\u001b[1m\n",
      "Epoch 66\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.103, WER=44.59, CER=11.46\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.769, WER=93.81, CER=51.62\n",
      "\u001b[1m\n",
      "Epoch 67\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.101, WER=44.70, CER=11.52\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.771, WER=94.10, CER=51.50\n",
      "\u001b[1m\n",
      "Epoch 68\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.099, WER=43.74, CER=11.17\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.776, WER=94.20, CER=51.44\n",
      "\u001b[1m\n",
      "Epoch 69\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.098, WER=44.17, CER=11.25\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.771, WER=93.51, CER=51.26\n",
      "\u001b[1m\n",
      "Epoch 70\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.096, WER=42.85, CER=10.79\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.773, WER=94.17, CER=50.99\n",
      "\u001b[1m\n",
      "Epoch 71\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.090, WER=40.92, CER=10.18\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.779, WER=94.98, CER=50.64\n",
      "\u001b[1m\n",
      "Epoch 72\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.088, WER=40.67, CER=9.98 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.783, WER=94.59, CER=50.66\n",
      "\u001b[1m\n",
      "Epoch 73\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.084, WER=38.68, CER=9.31\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.799, WER=95.47, CER=50.42\n",
      "\u001b[1m\n",
      "Epoch 74\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.081, WER=37.43, CER=9.06\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.814, WER=95.63, CER=50.67\n",
      "\u001b[1m\n",
      "Epoch 75\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.078, WER=35.87, CER=8.58\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.823, WER=94.67, CER=50.67\n",
      "\u001b[1m\n",
      "Epoch 76\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.077, WER=35.80, CER=8.55\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.831, WER=94.81, CER=50.77\n",
      "\u001b[1m\n",
      "Epoch 77\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.075, WER=35.46, CER=8.38\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.836, WER=94.67, CER=51.09\n",
      "\u001b[1m\n",
      "Epoch 78\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.073, WER=34.60, CER=8.14\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.835, WER=93.72, CER=50.87\n",
      "\u001b[1m\n",
      "Epoch 79\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.071, WER=33.92, CER=7.94\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.839, WER=94.13, CER=50.71\n",
      "\u001b[1m\n",
      "Epoch 80\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.069, WER=33.42, CER=7.77\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.843, WER=93.69, CER=50.56\n",
      "\u001b[1m\n",
      "Epoch 81\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.067, WER=32.06, CER=7.44\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.845, WER=94.86, CER=50.80\n",
      "\u001b[1m\n",
      "Epoch 82\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.065, WER=31.64, CER=7.27\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.859, WER=95.24, CER=50.98\n",
      "\u001b[1m\n",
      "Epoch 83\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.061, WER=30.20, CER=6.76\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.861, WER=95.67, CER=50.93\n",
      "\u001b[1m\n",
      "Epoch 84\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.060, WER=29.41, CER=6.62\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.868, WER=95.33, CER=50.87\n",
      "\u001b[1m\n",
      "Epoch 85\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.059, WER=29.00, CER=6.52\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.867, WER=95.36, CER=50.94\n",
      "\u001b[1m\n",
      "Epoch 86\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.057, WER=28.83, CER=6.40\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.877, WER=95.99, CER=51.17\n",
      "\u001b[1m\n",
      "Epoch 87\u001b[0m\n",
      "Running training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [63/63, 0.6 min(s)]: Loss=0.056, WER=27.82, CER=6.11\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.882, WER=95.23, CER=51.04\n",
      "\u001b[1m\n",
      "Epoch 88\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.054, WER=27.19, CER=5.91\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.884, WER=94.76, CER=51.30\n",
      "\u001b[1m\n",
      "Epoch 89\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.052, WER=26.18, CER=5.75\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.885, WER=95.09, CER=51.41\n",
      "\u001b[1m\n",
      "Epoch 90\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.050, WER=25.14, CER=5.47\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.893, WER=95.24, CER=51.46\n",
      "\u001b[1m\n",
      "Epoch 91\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.048, WER=24.20, CER=5.19\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.897, WER=94.65, CER=51.22\n",
      "\u001b[1m\n",
      "Epoch 92\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.047, WER=23.47, CER=5.02\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.911, WER=94.74, CER=51.14\n",
      "\u001b[1m\n",
      "Epoch 93\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.046, WER=23.55, CER=4.97\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.915, WER=95.56, CER=51.29\n",
      "\u001b[1m\n",
      "Epoch 94\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.044, WER=22.66, CER=4.78\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.922, WER=93.85, CER=51.10\n",
      "\u001b[1m\n",
      "Epoch 95\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.043, WER=21.56, CER=4.57\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.930, WER=93.82, CER=51.00\n",
      "\u001b[1m\n",
      "Epoch 96\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.042, WER=21.46, CER=4.49\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.946, WER=93.17, CER=51.27\n",
      "\u001b[1m\n",
      "Epoch 97\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.040, WER=20.52, CER=4.24\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.945, WER=93.18, CER=50.96\n",
      "\u001b[1m\n",
      "Epoch 98\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.040, WER=20.41, CER=4.28\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.946, WER=93.83, CER=50.99\n",
      "\u001b[1m\n",
      "Epoch 99\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.040, WER=20.60, CER=4.31\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.949, WER=93.27, CER=50.98\n",
      "\u001b[1m\n",
      "Epoch 100\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.039, WER=19.97, CER=4.17\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.957, WER=94.01, CER=50.87\n",
      "\u001b[1m\n",
      "Epoch 101\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.039, WER=19.90, CER=4.16\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.960, WER=93.83, CER=51.03\n",
      "\u001b[1m\n",
      "Epoch 102\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.039, WER=20.44, CER=4.26\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.973, WER=93.46, CER=51.03\n",
      "\u001b[1m\n",
      "Epoch 103\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.038, WER=20.09, CER=4.18\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.982, WER=93.76, CER=51.08\n",
      "\u001b[1m\n",
      "Epoch 104\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.038, WER=19.80, CER=4.13\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.988, WER=93.80, CER=51.19\n",
      "\u001b[1m\n",
      "Epoch 105\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.038, WER=20.50, CER=4.20\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.989, WER=93.99, CER=51.08\n",
      "\u001b[1m\n",
      "Epoch 106\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.039, WER=20.34, CER=4.25\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.995, WER=94.05, CER=51.02\n",
      "\u001b[1m\n",
      "Epoch 107\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.038, WER=20.58, CER=4.25\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.3 min(s)]: Loss=0.987, WER=94.75, CER=50.96\n",
      "\u001b[1m\n",
      "Epoch 108\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.037, WER=20.13, CER=4.13\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.994, WER=96.14, CER=51.07\n",
      "\u001b[1m\n",
      "Epoch 109\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.036, WER=19.60, CER=4.04\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.985, WER=96.90, CER=51.27\n",
      "\u001b[1m\n",
      "Epoch 110\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.037, WER=20.21, CER=4.15\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.981, WER=95.63, CER=51.17\n",
      "\u001b[1m\n",
      "Epoch 111\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.035, WER=18.38, CER=3.81\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.986, WER=96.28, CER=51.71\n",
      "\u001b[1m\n",
      "Epoch 112\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.035, WER=18.68, CER=3.85\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=0.996, WER=97.33, CER=51.85\n",
      "\u001b[1m\n",
      "Epoch 113\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.032, WER=17.52, CER=3.55\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.004, WER=97.14, CER=51.90\n",
      "\u001b[1m\n",
      "Epoch 114\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.031, WER=16.28, CER=3.29\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.009, WER=96.28, CER=51.55\n",
      "\u001b[1m\n",
      "Epoch 115\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.030, WER=15.76, CER=3.17\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.017, WER=96.88, CER=51.57\n",
      "\u001b[1m\n",
      "Epoch 116\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.028, WER=15.00, CER=2.97\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.030, WER=97.06, CER=51.81\n",
      "\u001b[1m\n",
      "Epoch 117\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.027, WER=14.02, CER=2.77\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.035, WER=96.13, CER=51.81\n",
      "\u001b[1m\n",
      "Epoch 118\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.026, WER=13.73, CER=2.75\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.047, WER=96.86, CER=51.92\n",
      "\u001b[1m\n",
      "Epoch 119\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.024, WER=12.71, CER=2.53\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.047, WER=95.39, CER=51.43\n",
      "\u001b[1m\n",
      "Epoch 120\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.023, WER=12.33, CER=2.42\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.047, WER=95.08, CER=51.29\n",
      "\u001b[1m\n",
      "Epoch 121\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.022, WER=11.55, CER=2.27\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.049, WER=95.66, CER=51.16\n",
      "\u001b[1m\n",
      "Epoch 122\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.021, WER=10.97, CER=2.11\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.065, WER=95.49, CER=51.44\n",
      "\u001b[1m\n",
      "Epoch 123\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.021, WER=10.49, CER=2.06\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.059, WER=95.83, CER=51.27\n",
      "\u001b[1m\n",
      "Epoch 124\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.020, WER=9.71, CER=1.89 \n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.070, WER=95.61, CER=51.14\n",
      "\u001b[1m\n",
      "Epoch 125\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.019, WER=9.33, CER=1.81\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.071, WER=96.17, CER=50.82\n",
      "\u001b[1m\n",
      "Epoch 126\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.018, WER=8.78, CER=1.70\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.076, WER=96.21, CER=50.85\n",
      "\u001b[1m\n",
      "Epoch 127\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.017, WER=8.53, CER=1.66\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.081, WER=96.64, CER=51.09\n",
      "\u001b[1m\n",
      "Epoch 128\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.017, WER=8.05, CER=1.55\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.082, WER=96.20, CER=50.77\n",
      "\u001b[1m\n",
      "Epoch 129\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.016, WER=7.94, CER=1.54\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.088, WER=95.81, CER=50.74\n",
      "\u001b[1m\n",
      "Epoch 130\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.016, WER=7.52, CER=1.43\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.099, WER=96.02, CER=50.85\n",
      "\u001b[1m\n",
      "Epoch 131\u001b[0m\n",
      "Running training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [63/63, 0.5 min(s)]: Loss=0.015, WER=7.40, CER=1.42\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.098, WER=95.92, CER=50.94\n",
      "\u001b[1m\n",
      "Epoch 132\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.015, WER=7.28, CER=1.39\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.105, WER=95.95, CER=51.03\n",
      "\u001b[1m\n",
      "Epoch 133\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.015, WER=7.14, CER=1.36\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.109, WER=94.96, CER=50.72\n",
      "\u001b[1m\n",
      "Epoch 134\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.014, WER=6.53, CER=1.23\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.111, WER=95.62, CER=50.72\n",
      "\u001b[1m\n",
      "Epoch 135\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.014, WER=6.68, CER=1.26\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.111, WER=95.52, CER=50.80\n",
      "\u001b[1m\n",
      "Epoch 136\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.014, WER=6.35, CER=1.22\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.120, WER=95.61, CER=50.85\n",
      "\u001b[1m\n",
      "Epoch 137\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.013, WER=6.16, CER=1.18\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.124, WER=95.32, CER=50.88\n",
      "\u001b[1m\n",
      "Epoch 138\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.013, WER=6.08, CER=1.16\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.125, WER=94.92, CER=51.00\n",
      "\u001b[1m\n",
      "Epoch 139\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.012, WER=5.70, CER=1.06\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.129, WER=95.84, CER=50.94\n",
      "\u001b[1m\n",
      "Epoch 140\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.013, WER=5.76, CER=1.10\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.132, WER=95.80, CER=51.04\n",
      "\u001b[1m\n",
      "Epoch 141\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.012, WER=5.61, CER=1.07\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.132, WER=95.51, CER=51.04\n",
      "\u001b[1m\n",
      "Epoch 142\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.012, WER=5.40, CER=1.02\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.136, WER=95.65, CER=51.04\n",
      "\u001b[1m\n",
      "Epoch 143\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.012, WER=5.63, CER=1.07\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.141, WER=95.36, CER=50.96\n",
      "\u001b[1m\n",
      "Epoch 144\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.012, WER=5.47, CER=1.04\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.143, WER=95.86, CER=51.10\n",
      "\u001b[1m\n",
      "Epoch 145\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.7 min(s)]: Loss=0.012, WER=5.28, CER=0.98\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.149, WER=95.96, CER=51.13\n",
      "\u001b[1m\n",
      "Epoch 146\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.5 min(s)]: Loss=0.011, WER=5.10, CER=0.96\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.146, WER=95.20, CER=51.13\n",
      "\u001b[1m\n",
      "Epoch 147\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.011, WER=5.08, CER=0.95\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.148, WER=94.64, CER=51.09\n",
      "\u001b[1m\n",
      "Epoch 148\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.011, WER=4.91, CER=0.93\n",
      "Running evaluation:\n",
      "Validation [169/169, 1.2 min(s)]: Loss=1.152, WER=94.61, CER=51.03\n",
      "\u001b[1m\n",
      "Epoch 149\u001b[0m\n",
      "Running training:\n",
      "Training [63/63, 0.6 min(s)]: Loss=0.010, WER=4.86, CER=0.90\n",
      "Running evaluation:\n",
      "Validation [79/169, 0.6 min(s)]: Loss=1.165, WER=95.99, CER=50.97\r"
     ]
    }
   ],
   "source": [
    "######################################### ASR not-pretained main script #################################################\n",
    "\n",
    "# Import general libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from datetime import datetime\n",
    "\n",
    "import editdistance\n",
    "\n",
    "# Import stuff from other ASR modules\n",
    "from asr.data import BaseDataset\n",
    "from asr.data.preprocessors import SpectrogramPreprocessor, TextPreprocessor\n",
    "from asr.modules import ASRModel\n",
    "from asr.utils.training import batch_to_tensor, epochs, Logger\n",
    "from asr.utils.text import greedy_ctc\n",
    "from asr.utils.metrics import ErrorRateTracker, LossTracker\n",
    "\n",
    "def get_Stats(ref_batch,hyp_batch):\n",
    "    ref_WER_splits = [rf.split() for rf in ref_batch]\n",
    "    hyp_WER_splits = [hp.split() for hp in hyp_batch]\n",
    "    ref_WER_lens = [len(rf_spl) for rf_spl in ref_WER_splits]\n",
    "    hyp_WER_lens = [len(hp_spl) for hp_spl in hyp_WER_splits]\n",
    "    \n",
    "    WER = [editdistance.eval(rf,hp) for rf, hp in zip(ref_WER_splits, hyp_WER_splits)]\n",
    "    \n",
    "    ref_CER_splits = [list(rf) for rf in ref_batch]\n",
    "    hyp_CER_splits = [list(hp) for hp in hyp_batch]\n",
    "    ref_CER_lens = [len(rf_spl) for rf_spl in ref_CER_splits]\n",
    "    hyp_CER_lens = [len(hp_spl) for hp_spl in hyp_CER_splits]\n",
    "    \n",
    "    CER = [editdistance.eval(rf,hp) for rf, hp in zip(ref_CER_splits, hyp_CER_splits)]\n",
    "    \n",
    "    return WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens\n",
    "\n",
    "\n",
    "\"\"\" Function: Train and test ASR model on input datasets\n",
    "    Input:    2 txt-files with IDs to training and test files. One observation consists of a transcript\n",
    "              (txt-feature) and audio file (wav-target).\n",
    "    Output:   Return best WER (validation) and save ASR model (model.pt) \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" Part 1: Load and preprocess data \"\"\"\n",
    "train_IDs = 'test-clean-cut'\n",
    "test_IDs = 'dev-clean'\n",
    "\n",
    "\n",
    "train_source = train_IDs\n",
    "val_source = test_IDs\n",
    "\n",
    "# BLACK BOX\n",
    "spec_preprocessor = SpectrogramPreprocessor(output_format='NFT', sample_rate=4000)\n",
    "text_preprocessor = TextPreprocessor()\n",
    "preprocessor = [spec_preprocessor, text_preprocessor]\n",
    "\n",
    "train_dataset = BaseDataset(source=train_source, preprocessor=preprocessor, sort_by=0)\n",
    "val_dataset = BaseDataset(source=val_source, preprocessor=preprocessor, sort_by=0)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(train_dataset, num_workers=4, pin_memory=True, collate_fn=train_dataset.collate, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, num_workers=4, pin_memory=True, collate_fn=val_dataset.collate, batch_size=16)\n",
    "\n",
    "\"\"\" Part 2: Setup model and loss \"\"\"\n",
    "# Create instance of model\n",
    "asr_model = ASRModel(input_size=40).cuda()\n",
    "print(asr_model)\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in asr_model.parameters() if p.requires_grad))\n",
    "\n",
    "# Define loss, optimizer and learning rate scheduler\n",
    "ctc_loss = nn.CTCLoss(reduction='none').cuda()#reduction='sum'\n",
    "optimizer = torch.optim.Adam(asr_model.parameters(), lr=3e-4)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=5e-5)\n",
    "\n",
    "\"\"\" Part 3: Train and evaluate model \"\"\"\n",
    "# Variables to create and store performance metrics\n",
    "wer_metric = ErrorRateTracker(word_based=True)\n",
    "cer_metric = ErrorRateTracker(word_based=False)\n",
    "ctc_metric = LossTracker()\n",
    "\n",
    "train_logger = Logger('Training', ctc_metric, wer_metric, cer_metric)\n",
    "val_logger = Logger('Validation', ctc_metric, wer_metric, cer_metric)\n",
    "\n",
    "def forward_pass(batch, training=False):\n",
    "    (x, x_sl), (y, y_sl) = batch_to_tensor(batch)  # For CPU: change 'cuda' to 'cpu'\n",
    "    \n",
    "    logits, output_sl = asr_model.forward(x, x_sl.cpu())\n",
    "    log_probs = F.log_softmax(logits, dim=2)\n",
    "    loss = ctc_loss(log_probs, y, output_sl, y_sl)\n",
    "    \n",
    "    hyp_encoded_batch = greedy_ctc(logits, output_sl)\n",
    "    hyp_batch = text_preprocessor.decode_batch(hyp_encoded_batch)\n",
    "    ref_batch = text_preprocessor.decode_batch(y, y_sl)\n",
    "    \n",
    "    wer_metric.update(ref_batch, hyp_batch)\n",
    "    cer_metric.update(ref_batch, hyp_batch)\n",
    "    ctc_metric.update(loss.sum().item(), weight=output_sl.sum().item())\n",
    "\n",
    "    if not training:\n",
    "        WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens = get_Stats(ref_batch,hyp_batch)\n",
    "        CTC_loss = [ctc.item() for ctc in loss]\n",
    "        return CTC_loss, WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "# Run 200 epochs\n",
    "model_name = f\"{train_IDs}vs{test_IDs}{datetime.now().strftime('Y%Y-m%m-d%d-H%H-M%M')}\" \n",
    "with open(f\"./results/{model_name}.csv\", \"a+\") as o_f:\n",
    "    for epoch in epochs(200):\n",
    "        # Set PyTorch in training mode\n",
    "        asr_model.train()\n",
    "\n",
    "        # Train model on training set\n",
    "        print(\"Running training:\")\n",
    "        for batch, files in train_logger(train_loader):\n",
    "            loss = forward_pass(batch, training=True)\n",
    "            optimizer.zero_grad()\n",
    "            loss.sum().backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Set PyTorch in test mode\n",
    "        asr_model.eval()\n",
    "\n",
    "        # Test model using test set\n",
    "        print(\"Running evaluation:\")\n",
    "        N = 1\n",
    "        wer_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, files in val_logger(val_loader):\n",
    "                CTC_loss, WER, ref_WER_lens, hyp_WER_lens, CER, ref_CER_lens, hyp_CER_lens = forward_pass(batch, training=False)\n",
    "                wer_sum += np.sum(WER)\n",
    "                N += len(WER)\n",
    "                for i, flac_file in enumerate(files):\n",
    "                    idx = os.path.splitext(os.path.basename(flac_file))[0]\n",
    "                    print(f\"{epoch}\\t{idx}\\t{CTC_loss[i]}\\t{WER[i]}\\t{ref_WER_lens[i]}\\t{hyp_WER_lens[i]}\\t{CER[i]}\\t{ref_CER_lens[i]}\\t{hyp_CER_lens[i]}\", file=o_f)\n",
    "\n",
    "        best_wer = np.inf\n",
    "        if wer_sum/N < best_wer:\n",
    "            best_wer = wer_sum/N\n",
    "            torch.save(asr_model.state_dict(), f\"./results/best_{model_name}.pt\")\n",
    "\n",
    "        if epoch >= 100:\n",
    "            lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-dominican",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "permanent-spray",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 26 11:36:20 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Tesla V1...  On   | 00000000:37:00.0 Off |                    0 |\n",
      "| N/A   73C    P0   216W / 250W |   4131MiB / 16160MiB |     92%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA Tesla V1...  On   | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    42W / 250W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     32686      C   ./darknet                        4127MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efficient-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "random.seed(12)\n",
    "os.environ['PYTHONHASHSEED'] = str(12)\n",
    "np.random.seed(12)\n",
    "torch.manual_seed(12)\n",
    "torch.cuda.manual_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "million-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## Tacotron 2 main script ##################################################\n",
    "\n",
    "# Import libraries\n",
    "from scipy.io.wavfile import write\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gen_tacotron2(transcript, trans_id, save_path, tacotron2, waveglow):\n",
    "    \"\"\" Function: Generate speech from 1 input transcript (collection of sentences) using Tacotron 2.\n",
    "        Input:    Transcript file with ID\n",
    "        Output:   Returns nothing but saves audio file (.wav) in folder 'gen_tacotron2' \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Preprocessing text\n",
    "    sequence = torch.tensor(tacotron2.text_to_sequence(transcript, ['english_cleaners']), device='cuda').unsqueeze(0)\n",
    "\n",
    "    # Generate speech using models\n",
    "    with torch.no_grad():\n",
    "        _, mel, _, _ = tacotron2.infer(sequence)\n",
    "        audio = waveglow.infer(mel)\n",
    "    audio_numpy = audio[0].data.cpu().numpy()\n",
    "    rate = 22050\n",
    "\n",
    "    write(save_path+'/'+trans_id+'.wav', rate, audio_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "saving-classics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /zhome/22/c/137477/.cache/torch/hub/nvidia_DeepLearningExamples_torchhub\n",
      "Using cache found in /zhome/22/c/137477/.cache/torch/hub/nvidia_DeepLearningExamples_torchhub\n",
      " 21%|██        | 540/2620 [08:08<30:48,  1.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Reached max decoder steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2620/2620 [43:11<00:00,  1.01it/s] \n"
     ]
    }
   ],
   "source": [
    "\"\"\" Program description: Generate speech using Tacotron 2. Tacotron 2 is pretrained on the entire LJSpeech dataset\n",
    "                         (see more at: https://github.com/NVIDIA/tacotron2). The speech is generated from a training\n",
    "                         split of the LibriSpeech dataset. Next, we train an ASR model on the generated dataset (see\n",
    "                         more at: https://github.com/borgholt/asr). The performance of the ASR model is evaluated on \n",
    "                         a test split of the LibriSpeech dataset. \n",
    "\n",
    "                         In result of this program is (1) a synthesized speech dataset, (2) a parameter save of the \n",
    "                         ASR model and (3) output of best ASR WER through stdout stream. \"\"\"\n",
    "\n",
    "\"\"\" Part 1: Define dataset file-IDs \"\"\"\n",
    "train_IDs = 'test-clean'           # LibriSpeech train (for generating)\n",
    "test_IDs = \"dev-clean\"           # LibriSpeech test  (for validating ASR)\n",
    "save_folder = 'tacotron2'\n",
    "\n",
    "if not os.path.exists(\"/work3/s194278/fagprojekt/LibriSpeech/\"+save_folder):\n",
    "    os.mkdir(\"/work3/s194278/fagprojekt/LibriSpeech/\"+save_folder)\n",
    "\n",
    "\"\"\" Part 2: Generate speech using Tacotron 2 \"\"\"\n",
    "# Get all transcript IDs and call the generator routine for each transcript one by one\n",
    "def split_line(line):\n",
    "    idx = line.split(\" \")[0]\n",
    "    trans = line[len(idx)+1:].rstrip()\n",
    "    return idx, trans\n",
    "\n",
    "with open('/work3/s194278/fagprojekt/LibriSpeech/'+train_IDs+'.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "trans_dict = dict([split_line(l) for l in lines])\n",
    "\n",
    "# Load pretrained Tacotron 2, transfer computation to GPU and set model in test mode\n",
    "tacotron2 = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_tacotron2')\n",
    "tacotron2 = tacotron2.to('cuda')\n",
    "tacotron2.eval()\n",
    "\n",
    "# Load pretrained WaveGlow, transfer computation to GPU and set model in test mode\n",
    "waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow')\n",
    "waveglow = waveglow.remove_weightnorm(waveglow)\n",
    "waveglow = waveglow.to('cuda')\n",
    "waveglow.eval()\n",
    "\n",
    "\n",
    "# Store transcript path (represents both .wav and .txt)\n",
    "save_path = \"/work3/s194278/fagprojekt/LibriSpeech/\" + save_folder\n",
    "\n",
    "for trans_id, transcript in tqdm(trans_dict.items()):\n",
    "    if len(transcript) > 128:\n",
    "        continue\n",
    "    # Generate speech and save\n",
    "    gen_tacotron2(transcript, trans_id, save_path, tacotron2, waveglow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-cooking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
